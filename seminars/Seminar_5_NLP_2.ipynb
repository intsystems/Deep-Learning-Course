{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNtLJlW4v5VF"
      },
      "source": [
        "# Оригинал - [Домашнее задание №1  с тренировок Яндекса](https://github.com/girafe-ai/ml-course/blob/25f_ml_trainings_4/homeworks/hw01_classification_and_attention/01_attention.ipynb)\n",
        "## Часть 1: Механизм внимания (Attention)\n",
        "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), https://t.me/s/rads_ai\n",
        "\n",
        "В данном задании вам предстоит детально рассмотреть механизм Attention (и реализовать несколько его вариантов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzvkbMOJWLOY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoYa0SSBWLOa"
      },
      "source": [
        "### Реализация Attention\n",
        "\n",
        "В данной задаче вам предстоит реализовать механизм Attention, в частности несколько способов подсчета attention scores. Конечно, в популярных фреймворках данный механизм уже реализован, но для лучшего понимания вам предстаит реализовать его с помощью `numpy`.\n",
        "\n",
        "Ваше задание в данной задаче: реализовать `additive` (аддитивный) и `multiplicative` (мультипликативный) варианты Attention. Для вашего удобства (и для примера) `dot product` attention (основанный на скалярном произведении) уже реализован.\n",
        "\n",
        "Детальное описание данных типов Attention доступно в лекционных слайдах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4AdVBlUWLOa"
      },
      "outputs": [],
      "source": [
        "decoder_hidden_state = np.array([7, 11, 4]).astype(float)[:, None]\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(decoder_hidden_state)\n",
        "plt.colorbar()\n",
        "plt.title(\"Decoder state\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBSzFknBWLOa"
      },
      "source": [
        "#### Dot product attention (пример реализации)\n",
        "Рассмотрим единственное состояние энкодера – вектор с размерностью `(n_hidden, 1)`, где `n_hidden = 3`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5icp7akIWLOa"
      },
      "outputs": [],
      "source": [
        "single_encoder_hidden_state = np.array([1, 5, 11]).astype(float)[:, None]\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(single_encoder_hidden_state)\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WphleTvUWLOb"
      },
      "source": [
        "Attention score между данными состояниями энкодера и декодера вычисляются просто как скалярное произведение:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEhDbTZWWLOb"
      },
      "outputs": [],
      "source": [
        "np.dot(decoder_hidden_state.T, single_encoder_hidden_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztRxnP_6WLOb"
      },
      "source": [
        "В общем случае состояний энкодера, конечно, несколько. Attention scores вычисляются с каждым из состояний энкодера:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNvqlgA8WLOb"
      },
      "outputs": [],
      "source": [
        "encoder_hidden_states = (\n",
        "    np.array([[1, 5, 11], [7, 4, 1], [8, 12, 2], [-9, 0, 1]]).astype(float).T\n",
        ")\n",
        "\n",
        "encoder_hidden_states"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkKXaYbhWLOb"
      },
      "source": [
        "Тогда для подсчета скалярных произведений между единственным состоянием декодера и всеми состояниями энкодера можно воспользоваться следующей функцией (которая по факту представляет собой просто матричное умножение и приведение типов):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcMlUd1aWLOb"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention_score(decoder_hidden_state, encoder_hidden_states):\n",
        "    \"\"\"\n",
        "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
        "\n",
        "    return: np.array of shape (1, n_states)\n",
        "        Array with dot product attention scores\n",
        "    \"\"\"\n",
        "    attention_scores = np.dot(decoder_hidden_state.T, encoder_hidden_states)\n",
        "    return attention_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WkRwfBTWLOc"
      },
      "outputs": [],
      "source": [
        "dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0noaX_iPWLOc"
      },
      "source": [
        "Для подсчета \"весов\" нам необходим Softmax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zafrHQTLWLOc"
      },
      "outputs": [],
      "source": [
        "def softmax(vector):\n",
        "    \"\"\"\n",
        "    vector: np.array of shape (n, m)\n",
        "\n",
        "    return: np.array of shape (n, m)\n",
        "        Matrix where softmax is computed for every row independently\n",
        "    \"\"\"\n",
        "    nice_vector = vector - vector.max()\n",
        "    exp_vector = np.exp(nice_vector)\n",
        "    exp_denominator = np.sum(exp_vector, axis=1)[:, np.newaxis]\n",
        "    softmax_ = exp_vector / exp_denominator\n",
        "    return softmax_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsI6virlWLOc"
      },
      "outputs": [],
      "source": [
        "weights_vector = softmax(\n",
        "    dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)\n",
        ")\n",
        "\n",
        "weights_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuc6VNdeWLOc"
      },
      "source": [
        "Наконец, воспользуемся данными весами и вычислим итоговый вектор, как и описано для dot product attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5S1WIvuGWLOc"
      },
      "outputs": [],
      "source": [
        "attention_vector = weights_vector.dot(encoder_hidden_states.T).T\n",
        "print(attention_vector)\n",
        "\n",
        "plt.figure(figsize=(2, 5))\n",
        "plt.pcolormesh(attention_vector, cmap=\"spring\")\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkIVEClWLOd"
      },
      "source": [
        "Данный вектор аккумулирует в себе информацию из всех состояний энкодера, взвешенную на основе близости к заданному состоянию декодера.\n",
        "\n",
        "Реализуем все вышеописанные преобразования в единой функции:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX8q3ktpWLOd"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(decoder_hidden_state, encoder_hidden_states):\n",
        "    \"\"\"\n",
        "    decoder_hidden_state: np.array of shape (n_features, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features, n_states)\n",
        "\n",
        "    return: np.array of shape (n_features, 1)\n",
        "        Final attention vector\n",
        "    \"\"\"\n",
        "    softmax_vector = softmax(\n",
        "        dot_product_attention_score(decoder_hidden_state, encoder_hidden_states)\n",
        "    )\n",
        "    attention_vector = softmax_vector.dot(encoder_hidden_states.T).T\n",
        "    return attention_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m1Idc0bWLOd"
      },
      "outputs": [],
      "source": [
        "assert (\n",
        "    attention_vector\n",
        "    == dot_product_attention(decoder_hidden_state, encoder_hidden_states)\n",
        ").all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D153kfcqWLOd"
      },
      "source": [
        "#### Multiplicative attention\n",
        "Ваша текущая задача: реализовать multiplicative attention.\n",
        "$$ e_i = \\mathbf{s}^TW_{mult}\\mathbf{h}_i $$\n",
        "\n",
        "Матрица весов `W_mult` задана ниже.\n",
        "Стоит заметить, что multiplicative attention позволяет работать с состояниями энкодера и декодера различных размерностей, поэтому состояния энкодера будут обновлены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIBqyQH0WLOd"
      },
      "outputs": [],
      "source": [
        "encoder_hidden_states_complex = (\n",
        "    np.array([[1, 5, 11, 4, -4], [7, 4, 1, 2, 2], [8, 12, 2, 11, 5], [-9, 0, 1, 8, 12]])\n",
        "    .astype(float)\n",
        "    .T\n",
        ")\n",
        "\n",
        "W_mult = np.array(\n",
        "    [\n",
        "        [-0.78, -0.97, -1.09, -1.79, 0.24],\n",
        "        [0.04, -0.27, -0.98, -0.49, 0.52],\n",
        "        [1.08, 0.91, -0.99, 2.04, -0.15],\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UktAjsjUWLOd"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG_HZsVYWLOd"
      },
      "source": [
        "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `multiplicative_attention`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItZIOOehWLOd"
      },
      "outputs": [],
      "source": [
        "def multiplicative_attention(decoder_hidden_state, encoder_hidden_states, W_mult):\n",
        "    \"\"\"\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    W_mult: np.array of shape (n_features_dec, n_features_enc)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    return attention_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEqtjLtDWLOe"
      },
      "source": [
        "#### Additive attention\n",
        "Теперь вам предстоит реализовать additive attention.\n",
        "\n",
        "$$ e_i = \\mathbf{v}^T \\text{tanh} (W_{add-enc} \\mathbf{h}_i + W_{add-dec} \\mathbf{s}) $$\n",
        "\n",
        "Матрицы весов `W_add_enc` и `W_add_dec` доступны ниже, как и вектор весов `v_add`. Для вычисления активации можно воспользоваться `np.tanh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j94VYOVeWLOe"
      },
      "outputs": [],
      "source": [
        "v_add = np.array([[-0.35, -0.58, 0.07, 1.39, -0.79, -1.78, -0.35]]).T\n",
        "\n",
        "W_add_enc = np.array(\n",
        "    [\n",
        "        [-1.34, -0.1, -0.38, 0.12, -0.34],\n",
        "        [-1.0, 1.28, 0.49, -0.41, -0.32],\n",
        "        [-0.39, -1.38, 1.26, 1.21, 0.15],\n",
        "        [-0.18, 0.04, 1.36, -1.18, -0.53],\n",
        "        [-0.23, 0.96, 1.02, 0.39, -1.26],\n",
        "        [-1.27, 0.89, -0.85, -0.01, -1.19],\n",
        "        [0.46, -0.12, -0.86, -0.93, -0.4],\n",
        "    ]\n",
        ")\n",
        "\n",
        "W_add_dec = np.array(\n",
        "    [\n",
        "        [-1.62, -0.02, -0.39],\n",
        "        [0.43, 0.61, -0.23],\n",
        "        [-1.5, -0.43, -0.91],\n",
        "        [-0.14, 0.03, 0.05],\n",
        "        [0.85, 0.51, 0.63],\n",
        "        [0.39, -0.42, 1.34],\n",
        "        [-0.47, -0.31, -1.34],\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5hwEjHMWLOe"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV6JBHioWLOe"
      },
      "source": [
        "Реализуйте подсчет attention согласно формулам и реализуйте итоговую функцию `additive_attention`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isPS4hSZWLOe"
      },
      "outputs": [],
      "source": [
        "def additive_attention(\n",
        "    decoder_hidden_state, encoder_hidden_states, v_add, W_add_enc, W_add_dec\n",
        "):\n",
        "    \"\"\"\n",
        "    decoder_hidden_state: np.array of shape (n_features_dec, 1)\n",
        "    encoder_hidden_states: np.array of shape (n_features_enc, n_states)\n",
        "    v_add: np.array of shape (n_features_int, 1)\n",
        "    W_add_enc: np.array of shape (n_features_int, n_features_enc)\n",
        "    W_add_dec: np.array of shape (n_features_int, n_features_dec)\n",
        "\n",
        "    return: np.array of shape (n_features_enc, 1)\n",
        "        Final attention vector\n",
        "    \"\"\"\n",
        "    # your code here\n",
        "    return attention_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJsCCtCiWLOe"
      },
      "source": [
        "Сдайте функции `multiplicative_attention` и `additive_attention` в контест.\n",
        "\n",
        "Не забудьте про импорт `numpy`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2KGH_i9WLOi"
      },
      "outputs": [],
      "source": [
        "# do not change the code in the block below\n",
        "# __________start of block__________\n",
        "\n",
        "np.save(\"submission_dict_hw08.npy\", out_dict, allow_pickle=True)\n",
        "print(\"File saved to `submission_dict_hw08.npy`\")\n",
        "# __________end of block__________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjGuVvb7WLOj"
      },
      "source": [
        "На этом задание завершено. Поздравляем!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLP_hw01_texts.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py3 Research",
      "language": "python",
      "name": "py3_research"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
