{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFTMJ2v-5YpN"
      },
      "source": [
        "# Seminar 2. Training networks in PyTorch\n",
        "\n",
        "On this seminar, we will train LeNet-5 on a MNIST dataset using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgba"
      ],
      "metadata": {
        "id": "0GE-ywENOwE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLswqviiNvx4"
      },
      "source": [
        "## Learning by example: Continuous XOR\n",
        "\n",
        "If we want to build a neural network in PyTorch, we could specify all our parameters (weight matrices, bias vectors) using `Tensors` (with `requires_grad=True`), ask PyTorch to calculate the gradients and then adjust the parameters. But things can quickly get cumbersome if we have a lot of parameters. In PyTorch, there is a package called `torch.nn` that makes building neural networks more convenient.\n",
        "\n",
        "We will introduce the libraries and all additional parts you might need to train a neural network in PyTorch, using a simple example classifier on a simple yet well known example: XOR. Given two binary inputs $x_1$ and $x_2$, the label to predict is $1$ if either $x_1$ or $x_2$ is $1$ while the other is $0$, or the label is $0$ in all other cases. The example became famous by the fact that a single neuron, i.e. a linear classifier, cannot learn this simple function.\n",
        "Hence, we will learn how to build a small neural network that can learn this function.\n",
        "To make it a little bit more interesting, we move the XOR into continuous space and introduce some gaussian noise on the binary inputs. Our desired separation of an XOR dataset could look as follows:\n",
        "\n",
        "<center style=\"width: 100%\"><img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/continuous_xor.svg\" width=\"350px\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RnFC6yyNvx4"
      },
      "source": [
        "### The model\n",
        "\n",
        "The package `torch.nn` defines a series of useful classes like linear networks layers, activation functions, loss functions etc. A full list can be found [here](https://pytorch.org/docs/stable/nn.html). In case you need a certain network layer, check the documentation of the package first before writing the layer yourself as the package likely contains the code for it already. We import it below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4swXSzNlNvx4"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7tNGyWANvx5"
      },
      "source": [
        "Additionally to `torch.nn`, there is also `torch.nn.functional`. It contains functions that are used in network layers. This is in contrast to `torch.nn` which defines them as `nn.Modules` (more on it below), and `torch.nn` actually uses a lot of functionalities from `torch.nn.functional`. Hence, the functional package is useful in many situations, and so we import it as well here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKo11ZQ-Nvx5"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUKjkKsGNvx5"
      },
      "source": [
        "#### nn.Module\n",
        "\n",
        "In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered to be a module itself as well. The basic template of a module is as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrZMIIniNvx5"
      },
      "outputs": [],
      "source": [
        "class MyModule(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Some init for my module\n",
        "        self.lin = nn.Linear\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Function for performing the calculation of the module.\n",
        "        return self.lin(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGYDAxWeNvx6"
      },
      "source": [
        "The forward function is where the computation of the module is taken place, and is executed when you call the module (`nn = MyModule(); nn(x)`). In the init function, we usually create the parameters of the module, using `nn.Parameter`, or defining other modules that are used in the forward function. The backward calculation is done automatically, but could be overwritten as well if wanted.\n",
        "\n",
        "#### Simple classifier\n",
        "We can now make use of the pre-defined modules in the `torch.nn` package, and define our own small neural network. We will use a minimal network with a input layer, one hidden layer with tanh as activation function, and a output layer. In other words, our networks should look something like this:\n",
        "\n",
        "<center width=\"100%\"><img src=\"https://uvadlc-notebooks.readthedocs.io/en/latest/_images/small_neural_network.svg\" width=\"300px\"></center>\n",
        "\n",
        "The input neurons are shown in blue, which represent the coordinates $x_1$ and $x_2$ of a data point. The hidden neurons including a tanh activation are shown in white, and the output neuron in red.\n",
        "In PyTorch, we can define this as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JojBG4QhNvx6"
      },
      "outputs": [],
      "source": [
        "class SimpleClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_hidden, num_outputs):\n",
        "        super().__init__()\n",
        "        # Initialize the modules we need to build the network\n",
        "        self.linear1 = nn.Linear(num_inputs, num_hidden)\n",
        "        self.act_fn = nn.Tanh()\n",
        "        self.linear2 = nn.Linear(num_hidden, num_outputs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Perform the calculation of the model to determine the prediction\n",
        "        x = self.linear1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ew7avDoNvx6"
      },
      "source": [
        "For the examples in this notebook, we will use a tiny neural network with two input neurons and four hidden neurons. As we perform binary classification, we will use a single output neuron. **Note that we do not apply a sigmoid on the output yet.** This is because other functions, especially the loss, are more efficient and precise to calculate on the original outputs instead of the sigmoid output. We will discuss the detailed reason later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWyFub4sNvx6"
      },
      "outputs": [],
      "source": [
        "model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "# Printing a module shows all its submodules\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiLK-aAMNvx6"
      },
      "source": [
        "Printing the model lists all submodules it contains. The parameters of a module can be obtained by using its `parameters()` functions, or `named_parameters()` to get a name to each parameter object. For our small neural network, we have the following parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMR3pXvPNvx7"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    print(f\"Parameter {name}, shape {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBlOhLSPNvx7"
      },
      "source": [
        "Each linear layer has a weight matrix of the shape `[output, input]`, and a bias of the shape `[output]`. The tanh activation function does not have any parameters. Note that parameters are only registered for `nn.Module` objects that are direct object attributes, i.e. `self.a = ...`. If you define a list of modules, the parameters of those are not registered for the outer module and can cause some issues when you try to optimize your module. There are alternatives, like `nn.ModuleList`, `nn.ModuleDict` and `nn.Sequential`, that allow you to have different data structures of modules. We will use them in a few later tutorials and explain them there."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsRJeOH9Nvx7"
      },
      "source": [
        "### The data\n",
        "\n",
        "PyTorch also provides a few functionalities to load the training and test data efficiently, summarized in the package `torch.utils.data`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX0ByD6BNvx7"
      },
      "outputs": [],
      "source": [
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw1EYvXTNvx8"
      },
      "source": [
        "The data package defines two classes which are the standard interface for handling data in PyTorch: `data.Dataset`, and `data.DataLoader`. The dataset class provides an uniform interface to access the training/test data, while the data loader makes sure to efficiently load and stack the data points from the dataset into batches during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10hy56WMNvx8"
      },
      "source": [
        "#### The dataset class\n",
        "\n",
        "The dataset class summarizes the basic functionality of a dataset in a natural way. To define a dataset in PyTorch, we simply specify two functions: `__getitem__`, and `__len__`. The get-item function has to return the $i$-th data point in the dataset, while the len function returns the size of the dataset. For the XOR dataset, we can define the dataset class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-EXbz6nWNvx9"
      },
      "outputs": [],
      "source": [
        "class XORDataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, size, std=0.1):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            size - Number of data points we want to generate\n",
        "            std - Standard deviation of the noise (see generate_continuous_xor function)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.size = size\n",
        "        self.std = std\n",
        "        self.generate_continuous_xor()\n",
        "\n",
        "    def generate_continuous_xor(self):\n",
        "        # Each data point in the XOR dataset has two variables, x and y, that can be either 0 or 1\n",
        "        # The label is their XOR combination, i.e. 1 if only x or only y is 1 while the other is 0.\n",
        "        # If x=y, the label is 0.\n",
        "        data = torch.randint(low=0, high=2, size=(self.size, 2), dtype=torch.float32)\n",
        "        label = (data.sum(dim=1) == 1).to(torch.long)\n",
        "        # To make it slightly more challenging, we add a bit of gaussian noise to the data points.\n",
        "        data += self.std * torch.randn(data.shape)\n",
        "\n",
        "        self.data = data\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        # Number of data point we have. Alternatively self.data.shape[0], or self.label.shape[0]\n",
        "        return self.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return the idx-th data point of the dataset\n",
        "        # If we have multiple things to return (data point and label), we can return them as tuple\n",
        "        data_point = self.data[idx]\n",
        "        data_label = self.label[idx]\n",
        "        return data_point, data_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7uiQW-RNvx_"
      },
      "source": [
        "Let's try to create such a dataset and inspect it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIUs44ScNvx_"
      },
      "outputs": [],
      "source": [
        "dataset = XORDataset(size=200)\n",
        "print(\"Size of dataset:\", len(dataset))\n",
        "print(\"Data point 0:\", dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A28FaZe5NvyA"
      },
      "source": [
        "To better relate to the dataset, we visualize the samples below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPl0XYARNvyA"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhCHpTp3NvyB"
      },
      "outputs": [],
      "source": [
        "visualize_samples(dataset.data, dataset.label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaDrhJq4NvyB"
      },
      "source": [
        "#### The data loader class\n",
        "\n",
        "The class `torch.utils.data.DataLoader` represents a Python iterable over a dataset with support for automatic batching, multi-process data loading and many more features. The data loader communicates with the dataset using the function `__getitem__`, and stacks its outputs as tensors over the first dimension to form a batch.\n",
        "In contrast to the dataset class, we usually don't have to define our own data loader class, but can just create an object of it with the dataset as input. Additionally, we can configure our data loader with the following input arguments (only a selection, see full list [here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)):\n",
        "\n",
        "* `batch_size`: Number of samples to stack per batch\n",
        "* `shuffle`: If True, the data is returned in a random order. This is important during training for introducing stochasticity.\n",
        "* `num_workers`: Number of subprocesses to use for data loading. The default, 0, means that the data will be loaded in the main process which can slow down training for datasets where loading a data point takes a considerable amount of time (e.g. large images). More workers are recommended for those, but can cause issues on Windows computers. For tiny datasets as ours, 0 workers are usually faster.\n",
        "* `pin_memory`: If True, the data loader will copy Tensors into CUDA pinned memory before returning them. This can save some time for large data points on GPUs. Usually a good practice to use for a training set, but not necessarily for validation and test to save memory on the GPU.\n",
        "* `drop_last`: If True, the last batch is dropped in case it is smaller than the specified batch size. This occurs when the dataset size is not a multiple of the batch size. Only potentially helpful during training to keep a consistent batch size.\n",
        "\n",
        "Let's create a simple data loader below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OVRNJ2wNvyB"
      },
      "outputs": [],
      "source": [
        "data_loader = data.DataLoader(dataset, batch_size=8, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptXrxy52NvyB"
      },
      "outputs": [],
      "source": [
        "# next(iter(...)) catches the first batch of the data loader\n",
        "# If shuffle is True, this will return a different batch every time we run this cell\n",
        "# For iterating over the whole dataset, we can simple use \"for batch in data_loader: ...\"\n",
        "data_inputs, data_labels = next(iter(data_loader))\n",
        "\n",
        "# The shape of the outputs are [batch_size, d_1,...,d_N] where d_1,...,d_N are the\n",
        "# dimensions of the data point returned from the dataset class\n",
        "print(\"Data inputs\", data_inputs.shape, \"\\n\", data_inputs)\n",
        "print(\"Data labels\", data_labels.shape, \"\\n\", data_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZZW3fz5NvyC"
      },
      "source": [
        "### Optimization\n",
        "\n",
        "After defining the model and the dataset, it is time to prepare the optimization of the model. During training, we will perform the following steps:\n",
        "\n",
        "1. Get a batch from the data loader\n",
        "2. Obtain the predictions from the model for the batch\n",
        "3. Calculate the loss based on the difference between predictions and labels\n",
        "4. Backpropagation: calculate the gradients for every parameter with respect to the loss\n",
        "5. Update the parameters of the model in the direction of the gradients\n",
        "\n",
        "We have seen how we can do step 1, 2 and 4 in PyTorch. Now, we will look at step 3 and 5."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsy_71nXNvyC"
      },
      "source": [
        "#### Loss modules\n",
        "\n",
        "We can calculate the loss for a batch by simply performing a few tensor operations as those are automatically added to the computation graph. For instance, for binary classification, we can use Binary Cross Entropy (BCE) which is defined as follows:\n",
        "\n",
        "$$L_{BCE} = -\\sum_i \\left[ y_i \\log x_i + (1 - y_i) \\log (1 - x_i) \\right]$$\n",
        "\n",
        "where $y$ are our labels, and $x$ our predictions, both in the range of $[0,1]$. However, PyTorch already provides a list of predefined loss functions which we can use (see [here](https://pytorch.org/docs/stable/nn.html#loss-functions) for a full list). For instance, for BCE, PyTorch has two modules: `nn.BCELoss()`, `nn.BCEWithLogitsLoss()`. While `nn.BCELoss` expects the inputs $x$ to be in the range $[0,1]$, i.e. the output of a sigmoid, `nn.BCEWithLogitsLoss` combines a sigmoid layer and the BCE loss in a single class. This version is numerically more stable than using a plain Sigmoid followed by a BCE loss because of the logarithms applied in the loss function. **Hence, it is adviced to use loss functions applied on \"logits\" where possible (remember to not apply a sigmoid on the output of the model in this case!).** For our model defined above, we therefore use the module `nn.BCEWithLogitsLoss`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fon5W8FnNvyC"
      },
      "outputs": [],
      "source": [
        "loss_module = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7LW-xyJNvyC"
      },
      "source": [
        "#### Stochastic Gradient Descent\n",
        "\n",
        "For updating the parameters, PyTorch provides the package `torch.optim` that has most popular optimizers implemented. We will discuss the specific optimizers and their differences later in the course, but will for now use the simplest of them: `torch.optim.SGD`. Stochastic Gradient Descent updates parameters by multiplying the gradients with a small constant, called learning rate, and subtracting those from the parameters (hence minimizing the loss). Therefore, we slowly move towards the direction of minimizing the loss. A good default value of the learning rate for a small network as ours is 0.1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7yLNlTINvyD"
      },
      "outputs": [],
      "source": [
        "# Input to the optimizer are the parameters of the model: model.parameters()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKKc3eQdNvyD"
      },
      "source": [
        "The optimizer provides two useful functions: `optimizer.step()`, and `optimizer.zero_grad()`. The step function updates the parameters based on the gradients as explained above. The function `optimizer.zero_grad()` sets the gradients of all parameters to zero. While this function seems less relevant at first, it is a crucial pre-step before performing backpropagation. If we call the `backward` function on the loss while the parameter gradients are non-zero from the previous batch, the new gradients would actually be added to the previous ones instead of overwriting them. This is done because a parameter might occur multiple times in a computation graph, and we need to sum the gradients in this case instead of replacing them. Hence, remember to call `optimizer.zero_grad()` before calculating the gradients of a batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqY2a6I2NvyD"
      },
      "source": [
        "### Training\n",
        "\n",
        "Finally, we are ready to train our model. As a first step, we create a slightly larger dataset and specify a data loader with a larger batch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeUC2oXANvyD"
      },
      "outputs": [],
      "source": [
        "train_dataset = XORDataset(size=2500)\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR9v6Z5zNvyE"
      },
      "source": [
        "Now, we can write a small training function. Remember our five steps: load a batch, obtain the predictions, calculate the loss, backpropagate, and update. Additionally, we have to push all data and model parameters to the device of our choice (GPU if available). For the tiny neural network we have, communicating the data to the GPU actually takes much more time than we could save from running the operation on GPU. For large networks, the communication time is significantly smaller than the actual runtime making a GPU crucial in these cases. Still, to practice, we will push the data to GPU here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E9cFk1oNvyE"
      },
      "outputs": [],
      "source": [
        "# Push model to device. Has to be only done once\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOG7L8Z5NvyE"
      },
      "source": [
        "In addition, we set our model to training mode. This is done by calling `model.train()`. There exist certain modules that need to perform a different forward step during training than during testing (e.g. BatchNorm and Dropout), and we can switch between them using `model.train()` and `model.eval()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ie9VS_fsNvyF"
      },
      "outputs": [],
      "source": [
        "def train_model(model, optimizer, data_loader, loss_module, num_epochs=100):\n",
        "    # Set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in tqdm(range(num_epochs)):\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
        "            data_inputs = data_inputs.to(device)\n",
        "            data_labels = data_labels.to(device)\n",
        "\n",
        "            ## Step 2: Run the model on the input data\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1) # Output is [Batch size, 1], but we want [Batch size]\n",
        "\n",
        "            ## Step 3: Calculate the loss\n",
        "            loss = loss_module(preds, data_labels.float())\n",
        "\n",
        "            ## Step 4: Perform backpropagation\n",
        "            # Before calculating the gradients, we need to ensure that they are all zero.\n",
        "            # The gradients would not be overwritten, but actually added to the existing ones.\n",
        "            optimizer.zero_grad()\n",
        "            # Perform backpropagation\n",
        "            loss.backward()\n",
        "\n",
        "            ## Step 5: Update the parameters\n",
        "            optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-h7vvbneNvyF"
      },
      "outputs": [],
      "source": [
        "train_model(model, optimizer, train_dataloader, loss_module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTjvF6OTNvyF"
      },
      "source": [
        "#### Saving a model\n",
        "\n",
        "After finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called `state_dict` from the model which contains all learnable parameters. For our simple model, the state dict contains the following entries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x054B37WNvyF"
      },
      "outputs": [],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhP7gB8uNvyG"
      },
      "source": [
        "To save the state dictionary, we can use `torch.save`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFwimiwANvyG"
      },
      "outputs": [],
      "source": [
        "# torch.save(object, filename). For the filename, any extension can be used\n",
        "torch.save(state_dict, \"our_model.tar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYUkuI0BNvyG"
      },
      "source": [
        "To load a model from a state dict, we use the function `torch.load` to load the state dict from the disk, and the module function `load_state_dict` to overwrite our parameters with the new values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqVFudWRNvyG"
      },
      "outputs": [],
      "source": [
        "# Load state dict from the disk (make sure it is the same name as above)\n",
        "state_dict = torch.load(\"our_model.tar\")\n",
        "\n",
        "# Create a new model and load the state\n",
        "new_model = SimpleClassifier(num_inputs=2, num_hidden=4, num_outputs=1)\n",
        "new_model.load_state_dict(state_dict)\n",
        "\n",
        "# Verify that the parameters are the same\n",
        "print(\"Original model\\n\", model.state_dict())\n",
        "print(\"\\nLoaded model\\n\", new_model.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwn6FgvONvyG"
      },
      "source": [
        "A detailed tutorial on saving and loading models in PyTorch can be found [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0YKkbEGNvyG"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Once we have trained a model, it is time to evaluate it on a held-out test set. As our dataset consist of randomly generated data points, we need to first create a test set with a corresponding data loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LiwFjCFNvyH"
      },
      "outputs": [],
      "source": [
        "test_dataset = XORDataset(size=500)\n",
        "# drop_last -> Don't drop the last batch although it is smaller than 128\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=128, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSLqU_hENvyH"
      },
      "source": [
        "As metric, we will use accuracy which is calculated as follows:\n",
        "\n",
        "$$acc = \\frac{\\#\\text{correct predictions}}{\\#\\text{all predictions}} = \\frac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "where TP are the true positives, TN true negatives, FP false positives, and FN the fale negatives.\n",
        "\n",
        "When evaluating the model, we don't need to keep track of the computation graph as we don't intend to calculate the gradients. This reduces the required memory and speed up the model. In PyTorch, we can deactivate the computation graph using `with torch.no_grad(): ...`. Remember to additionally set the model to eval mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6YFpooHNvyH"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, data_loader):\n",
        "    model.eval() # Set model to eval mode\n",
        "    true_preds, num_preds = 0., 0.\n",
        "\n",
        "    with torch.no_grad(): # Deactivate gradients for the following code\n",
        "        for data_inputs, data_labels in data_loader:\n",
        "\n",
        "            # Determine prediction of model on dev set\n",
        "            data_inputs, data_labels = data_inputs.to(device), data_labels.to(device)\n",
        "            preds = model(data_inputs)\n",
        "            preds = preds.squeeze(dim=1)\n",
        "\n",
        "            preds = torch.sigmoid(preds) # Sigmoid to map predictions between 0 and 1\n",
        "            pred_labels = (preds >= 0.5).long() # Binarize predictions to 0 and 1\n",
        "\n",
        "            # Keep records of predictions for the accuracy metric (true_preds=TP+TN, num_preds=TP+TN+FP+FN)\n",
        "            true_preds += (pred_labels == data_labels).sum()\n",
        "            num_preds += data_labels.shape[0]\n",
        "\n",
        "    acc = true_preds / num_preds\n",
        "    print(f\"Accuracy of the model: {100.0*acc:4.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hddpank4NvyH"
      },
      "outputs": [],
      "source": [
        "eval_model(model, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9XibNoBNvyH"
      },
      "source": [
        "If we trained our model correctly, we should see a score close to 100% accuracy. However, this is only possible because of our simple task, and unfortunately, we usually don't get such high scores on test sets of more complex tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-B9ZpzRNvyI"
      },
      "source": [
        "#### Visualizing classification boundaries\n",
        "\n",
        "To visualize what our model has learned, we can perform a prediction for every data point in a range of $[-0.5, 1.5]$, and visualize the predicted class as in the sample figure at the beginning of this section. This shows where the model has created decision boundaries, and which points would be classified as $0$, and which as $1$. We therefore get a background image out of blue (class 0) and orange (class 1). The spots where the model is uncertain we will see a blurry overlap. The specific code is less relevant compared to the output figure which should hopefully show us a clear separation of classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHYRG5b0NvyI"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad() # Decorator, same effect as \"with torch.no_grad(): ...\" over the whole function.\n",
        "def visualize_classification(model, data, label):\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        data = data.cpu().numpy()\n",
        "    if isinstance(label, torch.Tensor):\n",
        "        label = label.cpu().numpy()\n",
        "    data_0 = data[label == 0]\n",
        "    data_1 = data[label == 1]\n",
        "\n",
        "    fig = plt.figure(figsize=(4,4), dpi=500)\n",
        "    plt.scatter(data_0[:,0], data_0[:,1], edgecolor=\"#333\", label=\"Class 0\")\n",
        "    plt.scatter(data_1[:,0], data_1[:,1], edgecolor=\"#333\", label=\"Class 1\")\n",
        "    plt.title(\"Dataset samples\")\n",
        "    plt.ylabel(r\"$x_2$\")\n",
        "    plt.xlabel(r\"$x_1$\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Let's make use of a lot of operations we have learned above\n",
        "    model.to(device)\n",
        "    c0 = torch.Tensor(to_rgba(\"C0\")).to(device)\n",
        "    c1 = torch.Tensor(to_rgba(\"C1\")).to(device)\n",
        "    x1 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    x2 = torch.arange(-0.5, 1.5, step=0.01, device=device)\n",
        "    xx1, xx2 = torch.meshgrid(x1, x2, indexing='ij')  # Meshgrid function as in numpy\n",
        "    model_inputs = torch.stack([xx1, xx2], dim=-1)\n",
        "    preds = model(model_inputs)\n",
        "    preds = torch.sigmoid(preds)\n",
        "    output_image = (1 - preds) * c0[None,None] + preds * c1[None,None]  # Specifying \"None\" in a dimension creates a new one\n",
        "    output_image = output_image.cpu().numpy()  # Convert to numpy array. This only works for tensors on CPU, hence first push to CPU\n",
        "    plt.imshow(output_image, origin='lower', extent=(-0.5, 1.5, -0.5, 1.5))\n",
        "    plt.grid(False)\n",
        "    return fig\n",
        "\n",
        "_ = visualize_classification(model, dataset.data, dataset.label)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yiHI9uzNvyI"
      },
      "source": [
        "The decision boundaries might not look exactly as in the figure in the preamble of this section which can be caused by running it on CPU or a different GPU architecture. Nevertheless, the result on the accuracy metric should be the approximately the same."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialization"
      ],
      "metadata": {
        "id": "-feWcdJpQZi_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXFJLLTSNXOp"
      },
      "source": [
        "In this tutorial, we will review techniques for optimization and initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the following concepts: initialization and optimization.\n",
        "\n",
        "In the first half of the notebook, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks. In the second half, we focus on optimization comparing the optimizers SGD, SGD with Momentum, and Adam.\n",
        "\n",
        "Let's start with importing our standard libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6IWDlYENXOr"
      },
      "outputs": [],
      "source": [
        "## Standard libraries\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_JLITMgNXOt"
      },
      "source": [
        "We will use the same `set_seed` function as in Tutorial 3, as well as the path variables `DATASET_PATH` and `CHECKPOINT_PATH`. Adjust the paths if necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PfukTPvNXOt"
      },
      "outputs": [],
      "source": [
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "# Path to the folder where the pretrained models are saved\n",
        "CHECKPOINT_PATH = \"../saved_models/tutorial4\"\n",
        "\n",
        "# Function for setting the seed\n",
        "def set_seed(seed):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "set_seed(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgrLqkwnNXOv"
      },
      "source": [
        "In the last part of the notebook, we will train models using three different optimizers. The pretrained models for those are downloaded below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mPboVeGNXOw"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "# Github URL where saved models are stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial4/\"\n",
        "# Files to download\n",
        "pretrained_files = [\"FashionMNIST_SGD.config\",    \"FashionMNIST_SGD_results.json\",    \"FashionMNIST_SGD.tar\",\n",
        "                    \"FashionMNIST_SGDMom.config\", \"FashionMNIST_SGDMom_results.json\", \"FashionMNIST_SGDMom.tar\",\n",
        "                    \"FashionMNIST_Adam.config\",   \"FashionMNIST_Adam_results.json\",   \"FashionMNIST_Adam.tar\"   ]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for file_name in pretrained_files:\n",
        "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2R2Z8mdNXOx"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zzmkQeWNXOy"
      },
      "source": [
        "Throughout this notebook, we will use a deep fully connected network, similar to our previous tutorial. We will also again apply the network to FashionMNIST, so you can relate to the results of Tutorial 3.\n",
        "We start by loading the FashionMNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UR5BR6tLNXOz"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "\n",
        "# Transformations applied on each image => first make them a tensor, then normalize them with mean 0 and std 1\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.2861,), (0.3530,))\n",
        "                               ])\n",
        "\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "# Note that for actually training a model, we will use different data loaders\n",
        "# with a lower batch size.\n",
        "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
        "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
        "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytZCsoVRNXOz"
      },
      "source": [
        "In comparison to the previous tutorial, we have changed the parameters of the normalization transformation `transforms.Normalize`. The normalization is now designed to give us an expected mean of 0 and a standard deviation of 1 across pixels. This will be particularly relevant for the discussion about initialization we will look at below, and hence we change it here. It should be noted that in most classification tasks, both normalization techniques (between -1 and 1 or mean 0 and stddev 1) have shown to work well.\n",
        "We can calculate the normalization parameters by determining the mean and standard deviation on the original images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfVVRk2ONXOz"
      },
      "outputs": [],
      "source": [
        "print(\"Mean\", (train_dataset.data.float() / 255.0).mean().item())\n",
        "print(\"Std\", (train_dataset.data.float() / 255.0).std().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzj_niFTNXO0"
      },
      "source": [
        "We can verify the transformation by looking at the statistics of a single batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rjaJ3GlNXO0"
      },
      "outputs": [],
      "source": [
        "imgs, _ = next(iter(train_loader))\n",
        "print(f\"Mean: {imgs.mean().item():5.3f}\")\n",
        "print(f\"Standard deviation: {imgs.std().item():5.3f}\")\n",
        "print(f\"Maximum: {imgs.max().item():5.3f}\")\n",
        "print(f\"Minimum: {imgs.min().item():5.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oI4l_bINXO0"
      },
      "source": [
        "Note that the maximum and minimum are not 1 and -1 anymore, but shifted towards the positive values. This is because FashionMNIST contains a lot of black pixels, similar to MNIST.\n",
        "\n",
        "Next, we create a linear neural network. We use the same setup as in the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXuv2yv_NXO0"
      },
      "outputs": [],
      "source": [
        "class BaseNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
        "            input_size - Size of the input images in pixels\n",
        "            num_classes - Number of classes we want to predict\n",
        "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the network based on the specified hidden sizes\n",
        "        layers = []\n",
        "        layer_sizes = [input_size] + hidden_sizes\n",
        "        for layer_index in range(1, len(layer_sizes)):\n",
        "            layers += [nn.Linear(layer_sizes[layer_index-1], layer_sizes[layer_index]),\n",
        "                       act_fn]\n",
        "        layers += [nn.Linear(layer_sizes[-1], num_classes)]\n",
        "        self.layers = nn.ModuleList(layers) # A module list registers a list of modules as submodules (e.g. for parameters)\n",
        "\n",
        "        self.config = {\"act_fn\": act_fn.__class__.__name__, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes}\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBKeqtbNNXO0"
      },
      "source": [
        "For the activation functions, we make use of PyTorch's `torch.nn` library instead of implementing ourselves. However, we also define an `Identity` activation function. Although this activation function would significantly limit the network's modeling capabilities, we will use it in the first steps of our discussion about initialization (for simplicity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JjwZOwWNXO1"
      },
      "outputs": [],
      "source": [
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "act_fn_by_name = {\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"identity\": Identity\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgBjeJysNXO1"
      },
      "source": [
        "Finally, we define a few plotting functions that we will use for our discussions. These functions help us to (1) visualize the weight/parameter distribution inside a network, (2) visualize the gradients that the parameters at different layers receive, and (3) the activations, i.e. the output of the linear layers. The detailed code is not important, but feel free to take a closer look if interested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71Fub5ZqNXO1"
      },
      "outputs": [],
      "source": [
        "##############################################################\n",
        "\n",
        "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n",
        "    columns = len(val_dict)\n",
        "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
        "    fig_index = 0\n",
        "    for key in sorted(val_dict.keys()):\n",
        "        key_ax = ax[fig_index%columns]\n",
        "        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n",
        "                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())>1e-8)) # Only plot kde if there is variance\n",
        "        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)>1 else \"\"))\n",
        "        if xlabel is not None:\n",
        "            key_ax.set_xlabel(xlabel)\n",
        "        fig_index += 1\n",
        "    fig.subplots_adjust(wspace=0.4)\n",
        "    return fig\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_weight_distribution(model, color=\"C0\"):\n",
        "    weights = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.endswith(\".bias\"):\n",
        "            continue\n",
        "        key_name = f\"Layer {name.split('.')[1]}\"\n",
        "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
        "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_gradients(model, color=\"C0\", print_variance=False):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        net - Object of class BaseNetwork\n",
        "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
        "    imgs, labels = next(iter(small_loader))\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    # Pass one batch through the network, and calculate the gradients for the weights\n",
        "    model.zero_grad()\n",
        "    preds = model(imgs)\n",
        "    loss = F.cross_entropy(preds, labels) # Same as nn.CrossEntropyLoss, but as a function instead of module\n",
        "    loss.backward()\n",
        "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
        "    grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n",
        "    model.zero_grad()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n",
        "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if print_variance:\n",
        "        for key in sorted(grads.keys()):\n",
        "            print(f\"{key} - Variance: {np.var(grads[key])}\")\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_activations(model, color=\"C0\", print_variance=False):\n",
        "    model.eval()\n",
        "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
        "    imgs, labels = next(iter(small_loader))\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    # Pass one batch through the network, and calculate the gradients for the weights\n",
        "    feats = imgs.view(imgs.shape[0], -1)\n",
        "    activations = {}\n",
        "    with torch.no_grad():\n",
        "        for layer_index, layer in enumerate(model.layers):\n",
        "            feats = layer(feats)\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                activations[f\"Layer {layer_index}\"] = feats.view(-1).detach().cpu().numpy()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n",
        "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if print_variance:\n",
        "        for key in sorted(activations.keys()):\n",
        "            print(f\"{key} - Variance: {np.var(activations[key])}\")\n",
        "\n",
        "\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LwSizyFNXO1"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "Before starting our discussion about initialization, it should be noted that there exist many very good blog posts about the topic of neural network initialization (for example [deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/), or a more [math-focused blog post](https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K)). In case something remains unclear after this tutorial, we recommend skimming through these blog posts as well.\n",
        "\n",
        "When initializing a neural network, there are a few properties we would like to have. First, the variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons. If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value. Similarly, if the variance increases, it is likely to explode (i.e. head to infinity) the deeper we design our model. The second property we look out for in initialization techniques is a gradient distribution with equal variance across layers. If the first layer receives much smaller gradients than the last layer, we will have difficulties in choosing an appropriate learning rate.\n",
        "\n",
        "As a starting point for finding a good method, we will analyze different initialization based on our linear neural network with no activation function (i.e. an identity). We do this because initializations depend on the specific activation function used in the network, and we can adjust the initialization schemes later on for our specific choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8vpzFPBNXO2"
      },
      "outputs": [],
      "source": [
        "model = BaseNetwork(act_fn=Identity()).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nse2HW8nNXO2"
      },
      "source": [
        "### Constant initialization\n",
        "\n",
        "The first initialization we can consider is to initialize all weights with the same constant value. Intuitively, setting all weights to zero is not a good idea as the propagated gradient will be zero. However, what happens if we set all weights to a value slightly larger or smaller than 0? To find out, we can implement a function for setting all parameters below and visualize the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Acdfw6lrNXO2"
      },
      "outputs": [],
      "source": [
        "def const_init(model, c=0.0):\n",
        "    for name, param in model.named_parameters():\n",
        "        param.data.fill_(c)\n",
        "\n",
        "const_init(model, c=0.005)\n",
        "visualize_gradients(model)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxkstGwBNXO2"
      },
      "source": [
        "As we can see, only the first and the last layer have diverse gradient distributions while the other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it). Having the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters. This would make our layer useless and reduce our effective number of parameters to 1. Thus, we cannot use a constant initialization to train our networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VjoHduhNXO3"
      },
      "source": [
        "### Constant variance\n",
        "\n",
        "From the experiment above, we have seen that a constant value is not working. So instead, how about we initialize the parameters by randomly sampling from a distribution like a Gaussian? The most intuitive way would be to choose one variance that is used for all layers in the network. Let's implement it below, and visualize the activation distribution across layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRrHkpl3NXO3"
      },
      "outputs": [],
      "source": [
        "def var_init(model, std=0.01):\n",
        "    for name, param in model.named_parameters():\n",
        "        param.data.normal_(std=std)\n",
        "\n",
        "var_init(model, std=0.01)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDXgzPXqNXO3"
      },
      "source": [
        "The variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer. Alternatively, we could use a higher standard deviation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO6KOqmxNXO3"
      },
      "outputs": [],
      "source": [
        "var_init(model, std=0.1)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-kgryLrNXO3"
      },
      "source": [
        "With a higher standard deviation, the activations are likely to explode. You can play around with the specific standard deviation values, but it will be hard to find one that gives us a good activation distribution across layers and is very specific to our model. If we would change the hidden sizes or number of layers, you would have to search all over again, which is neither efficient nor recommended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plflXPWVNXO4"
      },
      "source": [
        "### How to find appropriate initialization values\n",
        "\n",
        "From our experiments above, we have seen that we need to sample the weights from a distribution, but are not sure which one exactly. As a next step, we will try to find the optimal initialization from the perspective of the activation distribution. For this, we state two requirements:\n",
        "\n",
        "1. The mean of the activations should be zero\n",
        "2. The variance of the activations should stay the same across every layer\n",
        "\n",
        "Suppose we want to design an initialization for the following layer: $y=Wx+b$ with $y\\in\\mathbb{R}^{d_y}$, $x\\in\\mathbb{R}^{d_x}$. Our goal is that the variance of each element of $y$ is the same as the input, i.e. $\\text{Var}(y_i)=\\text{Var}(x_i)=\\sigma_x^{2}$, and that the mean is zero. We assume $x$ to also have a mean of zero, because, in deep neural networks, $y$ would be the input of another layer. This requires the bias and weight to have an expectation of 0. Actually, as $b$ is a single element per output neuron and is constant across different inputs, we set it to 0 overall.\n",
        "\n",
        "Next, we need to calculate the variance with which we need to initialize the weight parameters.\n",
        "Along the calculation, we will need the following variance rule: given two independent variables, the variance of their product is $\\text{Var}(X\\cdot Y) = \\mathbb{E}(Y)^2\\text{Var}(X) + \\mathbb{E}(X)^2\\text{Var}(Y) + \\text{Var}(X)\\text{Var}(Y) = \\mathbb{E}(Y^2)\\mathbb{E}(X^2)-\\mathbb{E}(Y)^2\\mathbb{E}(X)^2$ ($X$ and $Y$ are not refering to $x$ and $y$, but any random variable).\n",
        "\n",
        "The needed variance of the weights, $\\text{Var}(w_{ij})$, is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    y_i & = \\sum_{j} w_{ij}x_{j}\\hspace{10mm}\\text{Calculation of a single output neuron without bias}\\\\\n",
        "    \\text{Var}(y_i) = \\sigma_x^{2} & = \\text{Var}\\left(\\sum_{j} w_{ij}x_{j}\\right)\\\\\n",
        "    & = \\sum_{j} \\text{Var}(w_{ij}x_{j}) \\hspace{10mm}\\text{Inputs and weights are independent of each other}\\\\\n",
        "    & = \\sum_{j} \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance rule (see above) with expectations being zero}\\\\\n",
        "    & = d_x \\cdot \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance equal for all $d_x$ elements}\\\\\n",
        "    & = \\sigma_x^{2} \\cdot d_x \\cdot \\text{Var}(w_{ij})\\\\\n",
        "    \\Rightarrow \\text{Var}(w_{ij}) = \\sigma_{W}^2 & = \\frac{1}{d_x}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Thus, we should initialize the weight distribution with a variance of the inverse of the input dimension $d_x$. Let's implement it below and check whether this holds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHVyx2I0NXO4"
      },
      "outputs": [],
      "source": [
        "def equal_var_init(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.endswith(\".bias\"):\n",
        "            param.data.fill_(0)\n",
        "        else:\n",
        "            param.data.normal_(std=1.0/math.sqrt(param.shape[1]))\n",
        "\n",
        "equal_var_init(model)\n",
        "visualize_weight_distribution(model)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naB9ZNetNXO4"
      },
      "source": [
        "As we expected, the variance stays indeed constant across layers. Note that our initialization does not restrict us to a normal distribution, but allows any other distribution with a mean of 0 and variance of $1/d_x$. You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.\n",
        "\n",
        "Besides the variance of the activations, another variance we would like to stabilize is the one of the gradients. This ensures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from $\\Delta x=W\\Delta y$, and come to the conclusion that we should initialize our layers with $1/d_y$ where $d_y$ is the number of output neurons. You can do the calculation as a practice, or check a thorough explanation in [this blog post](https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K). As a compromise between both constraints, [Glorot and Bengio (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi) proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:\n",
        "\n",
        "$$W\\sim N\\left(0,\\frac{2}{d_x+d_y}\\right)$$\n",
        "\n",
        "If we use a uniform distribution, we would initialize the weights with:\n",
        "\n",
        "$$W\\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}, \\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}\\right]$$\n",
        "\n",
        "Let's shortly implement it and validate its effectiveness:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oa9Ifm-3NXO4"
      },
      "outputs": [],
      "source": [
        "def xavier_init(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.endswith(\".bias\"):\n",
        "            param.data.fill_(0)\n",
        "        else:\n",
        "            bound = math.sqrt(6)/math.sqrt(param.shape[0]+param.shape[1])\n",
        "            param.data.uniform_(-bound, bound)\n",
        "\n",
        "xavier_init(model)\n",
        "visualize_gradients(model, print_variance=True)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPprtPQ7NXO5"
      },
      "source": [
        "We see that the Xavier initialization balances the variance of gradients and activations. Note that the significantly higher variance for the output layer is due to the large difference of input and output dimension ($128$ vs $10$). However, we currently assumed the activation function to be linear. So what happens if we add a non-linearity? In a tanh-based network, a common assumption is that for small values during the initial steps in training, the $\\tanh$ works as a linear function such that we don't have to adjust our calculation. We can check if that is the case for us as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZt0nWzmNXO5"
      },
      "outputs": [],
      "source": [
        "model = BaseNetwork(act_fn=nn.Tanh()).to(device)\n",
        "xavier_init(model)\n",
        "visualize_gradients(model, print_variance=True)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSjmMEvONXO5"
      },
      "source": [
        "Although the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values. Therefore, our variance will stabilize around 0.25 if we would go even deeper. Hence, we can conclude that the Xavier initialization works well for Tanh networks. But what about ReLU networks? Here, we cannot take the previous assumption of the non-linearity becoming linear for small values. The ReLU activation function sets (in expectation) half of the inputs to 0 so that also the expectation of the input is not zero. However, as long as the expectation of $W$ is zero and $b=0$, the expectation of the output is zero. The part where the calculation of the ReLU initialization differs from the identity is when determining $\\text{Var}(w_{ij}x_{j})$:\n",
        "\n",
        "$$\\text{Var}(w_{ij}x_{j})=\\underbrace{\\mathbb{E}[w_{ij}^2]}_{=\\text{Var}(w_{ij})}\\mathbb{E}[x_{j}^2]-\\underbrace{\\mathbb{E}[w_{ij}]^2}_{=0}\\mathbb{E}[x_{j}]^2=\\text{Var}(w_{ij})\\mathbb{E}[x_{j}^2]$$\n",
        "\n",
        "If we assume now that $x$ is the output of a ReLU activation (from a previous layer, $x=max(0,\\tilde{y})$), we can calculate the expectation as follows:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    \\mathbb{E}[x^2] & =\\mathbb{E}[\\max(0,\\tilde{y})^2]\\\\\n",
        "                    & =\\frac{1}{2}\\mathbb{E}[{\\tilde{y}}^2]\\hspace{2cm}\\tilde{y}\\text{ is zero-centered and symmetric}\\\\\n",
        "                    & =\\frac{1}{2}\\text{Var}(\\tilde{y})\n",
        "\\end{split}$$\n",
        "\n",
        "Thus, we see that we have an additional factor of 1/2 in the equation, so that our desired weight variance becomes $2/d_x$. This gives us the Kaiming initialization (see [He, K. et al. (2015)](https://arxiv.org/pdf/1502.01852.pdf)). Note that the Kaiming initialization does not use the harmonic mean between input and output size. In their paper (Section 2.2, Backward Propagation, last paragraph), they argue that using $d_x$ or $d_y$ both lead to stable gradients throughout the network, and only depend on the overall input and output size of the network. Hence, we can use here only the input $d_x$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zamly1PjNXO6"
      },
      "outputs": [],
      "source": [
        "def kaiming_init(model):\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.endswith(\".bias\"):\n",
        "            param.data.fill_(0)\n",
        "        elif name.startswith(\"layers.0\"): # The first layer does not have ReLU applied on its input\n",
        "            param.data.normal_(0, 1/math.sqrt(param.shape[1]))\n",
        "        else:\n",
        "            param.data.normal_(0, math.sqrt(2)/math.sqrt(param.shape[1]))\n",
        "\n",
        "model = BaseNetwork(act_fn=nn.ReLU()).to(device)\n",
        "kaiming_init(model)\n",
        "visualize_gradients(model, print_variance=True)\n",
        "visualize_activations(model, print_variance=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variance stays stable across layers. We can conclude that the Kaiming initialization indeed works well for ReLU-based networks. Note that for Leaky-ReLU etc., we have to slightly adjust the factor of $2$ in the variance as half of the values are not set to zero anymore. PyTorch provides a function to calculate this factor for many activation function, see `torch.nn.init.calculate_gain` ([link](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.calculate_gain))."
      ],
      "metadata": {
        "id": "LuypahlqQ3Wt"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}