{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PnA6ygzBzx2m",
        "yUd0XH9wLUKb",
        "ZwzxMMKpLWyi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Семинар к лекции 4 курса Deep Learning. NLP Part 1\n",
        "\n",
        "На семинаре предлагается рассмотреть этапы преобразования текстов в эмбеддинги с помощью алгоритмов One-Hot, Bag-of-Words, TF-IDF и сравнить методы как между собой, так и с их табличными реализациями.\n",
        "\n"
      ],
      "metadata": {
        "id": "TkQqGXX7ObeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Данные и библиотеки\n"
      ],
      "metadata": {
        "id": "1rzFH89rLI-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import time\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.utils import shuffle\n",
        "import pymorphy3\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import time\n",
        "import spacy\n",
        "import pymorphy3\n",
        "from pymystem3 import Mystem\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "EA8g3KWA6g1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве данных для анализа предлагается взять интересный датасет с анекдотами на русском языке."
      ],
      "metadata": {
        "id": "r7fjcqgvPQ6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Koziev/NLP_Datasets/raw/refs/heads/master/Conversations/Data/extract_dialogues_from_anekdots.tar.xz\n",
        "!tar -xf extract_dialogues_from_anekdots.tar.xz"
      ],
      "metadata": {
        "id": "VRiPNyk5YCBi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "with open(\"extract_dialogues_from_anekdots.txt\", encoding=\"utf-8\") as file:\n",
        "    data = file.read()\n",
        "    data = data.split('\\n\\n\\n\\n')\n",
        "data[69]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vM3rZ_YeTFfo",
        "outputId": "c339ca9b-c47d-4e62-f880-e4a48a347e4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'- Анка, ты не знаешь, почему у Петьки волосы на голове такие пышные и кудрявые?\\n- А он, Василий Иваныч, их яйцами натирает.\\n- Во, акробат!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm\n",
        "!pip install pymorphy3 pymystem3"
      ],
      "metadata": {
        "id": "cZ-Hums8fxs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка\n",
        "\n",
        "Очень важным этапом при работе с текстами является предобработка входных данных. Здесь и разбиение предложений (и возможно слов) на токены, приведение их к стандартной форме, удаление стоп-слов и лишних символов и прочее.\n",
        "\n",
        "Для русского языка наиболее популярными являются 4 библиотеки:\n",
        "  - pymorphy3   -  https://pypi.org/project/pymorphy3/\n",
        "  - Mystem      -  https://yandex.ru/dev/mystem/\n",
        "  - SpaCy       -  https://spacy.io/models/ru/\n",
        "  - natasha     -  https://pypi.org/project/natasha/\n",
        "\n",
        "Предлагается разобраться с основными методами этих библиотек и сравнить их между собой по качеству и скорости работы."
      ],
      "metadata": {
        "id": "-qkOVUqtcI72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy3.MorphAnalyzer()  # https://pypi.org/project/pymorphy3/\n",
        "\n",
        "morph.parse('бежит')[0].normal_form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rq5OLtF3dr-T",
        "outputId": "aa016704-1240-4597-fb26-4dd108428a52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'бежать'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mystem = Mystem()  # https://yandex.ru/dev/mystem/\n",
        "mystem.lemmatize('бежит')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBqvr_SIe3P2",
        "outputId": "8baaf41d-9dbf-4549-a668-d0efdca569c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['бежать', '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "doc = nlp('бежит')\n",
        "doc  # https://spacy.io/models/ru/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91l6wSgRfgRW",
        "outputId": "2805b896-728a-4ec9-8773-f42ecd3609c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "бежит"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.org/project/natasha/"
      ],
      "metadata": {
        "id": "Wz4OguBjhdsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    cleaned_text = re.sub(r'[^а-яА-ЯёЁ\\s0-9S:\\[\\].,-]', ' ', text)\n",
        "    tokens = cleaned_text.lower().split()\n",
        "    tokens = [token for token in tokens if len(token) > 2]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "def lemmatize_spacy(tokens):\n",
        "    nlp = spacy.load(\"ru_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
        "    text_to_process = ' '.join(tokens)\n",
        "    doc = nlp(text_to_process)\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    return lemmas\n",
        "\n",
        "def lemmatize_pymystem(tokens):\n",
        "    mystem = Mystem()\n",
        "    text_to_process = ' '.join(tokens)\n",
        "    lemmas_list = mystem.lemmatize(text_to_process)\n",
        "    lemmas = [lemma.strip() for lemma in lemmas_list if lemma.strip() and not lemma.isspace()]\n",
        "    return lemmas\n",
        "\n",
        "def lemmatize_pymorphy3(tokens):\n",
        "    morph = pymorphy3.MorphAnalyzer()\n",
        "    lemmas = []\n",
        "    for token in tokens:\n",
        "        parsed = morph.parse(token)[0]\n",
        "        lemmas.append(parsed.normal_form)\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "def compare_lemmatizers(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    print(f\"Предобработанные токены: {tokens}\\n\")\n",
        "\n",
        "    methods = {\n",
        "        'spaCy': lemmatize_spacy,\n",
        "        'PyMystem3': lemmatize_pymystem,\n",
        "        'PyMorphy3': lemmatize_pymorphy3\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    times = {}\n",
        "\n",
        "    for name, func in methods.items():\n",
        "        start_time = time.time()\n",
        "        results[name] = func(tokens)\n",
        "        end_time = time.time()\n",
        "        times[name] = end_time - start_time\n",
        "    print(\"Результаты лемматизации:\")\n",
        "    for name, lemmas in results.items():\n",
        "        print(f\"{name}: {lemmas}\")\n",
        "        print(f\"Время выполнения: {times[name]:.4f} секунд\\n\")\n",
        "\n",
        "def test_textes(test_text):\n",
        "    print(\"Оригинальный текст:\", test_text)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    compare_lemmatizers(test_text)"
      ],
      "metadata": {
        "id": "Lh_uxO_ydvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot + BoW\n",
        "\n",
        "На занятии предлагалось разобраться с реализацией tf-idf и дополнив код сравнить с реализацией из библиотеки\n",
        "\n"
      ],
      "metadata": {
        "id": "PnA6ygzBzx2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте реализуем one-hot:\n",
        "1. Проходим по всем словам и подсчитываем частоты\n",
        "2. Если есть ограничение на размер словаря - оставляем топ N\n",
        "3. Создаем словарь слово->индекс"
      ],
      "metadata": {
        "id": "VvrZUZuqgYaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_one_hot_encoding(texts, max_features=None):\n",
        "    \"\"\"\n",
        "    Простая реализация One-Hot Encoding для текстовых данных\n",
        "\n",
        "    Args:\n",
        "        texts: список текстов для обработки\n",
        "        max_features: максимальное количество слов для включения в словарь\n",
        "    \"\"\"\n",
        "    # Создаем словарь для подсчета частот слов\n",
        "    word_freq = defaultdict(int)\n",
        "\n",
        "    # Разбиваем тексты на слова и подсчитываем частоты\n",
        "    for text in texts:\n",
        "        for word in text.lower().split():\n",
        "            word_freq[word] += 1\n",
        "\n",
        "    # Сортируем слова по частоте и выбираем top-N\n",
        "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    if max_features:\n",
        "        sorted_words = sorted_words[:max_features]\n",
        "\n",
        "    # Создаем словарь слово->индекс\n",
        "    word_to_index = {word: idx for idx, (word, _) in enumerate(sorted_words)}\n",
        "    vocab_size = len(word_to_index)\n",
        "\n",
        "    print(f\"Размер словаря: {vocab_size} слов\")\n",
        "\n",
        "    # Создаем one-hot матрицу\n",
        "    one_hot_matrix = []\n",
        "    for text in texts:\n",
        "        # Вектор нулей размером с словарь\n",
        "        vector = np.zeros(vocab_size)\n",
        "        for word in text.lower().split():\n",
        "            if word in word_to_index:\n",
        "                vector[word_to_index[word]] = 1\n",
        "        one_hot_matrix.append(vector)\n",
        "\n",
        "    return np.array(one_hot_matrix), word_to_index\n",
        "\n",
        "# Применяем нашу реализацию\n",
        "print(\"=== Кастомная реализация One-Hot Encoding ===\")\n",
        "start_time = time.time()\n",
        "custom_encoded, vocab = custom_one_hot_encoding(data[:10000])\n",
        "custom_time = time.time() - start_time\n",
        "\n",
        "print(f\"Время выполнения: {custom_time:.4f} секунд\")\n",
        "print(f\"Размер матрицы: {custom_encoded.shape}\")\n",
        "print(f\"Память: {custom_encoded.nbytes} байт\")\n",
        "\n",
        "####################################################\n",
        "\n",
        "# Используем CountVectorizer с binary=True для one-hot представления\n",
        "print(\"\\n=== Реализация Scikit-Learn ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# CountVectorizer с binary=True создает бинарные признаки (one-hot)\n",
        "sklearn_vectorizer = CountVectorizer(binary=True)\n",
        "sklearn_encoded = sklearn_vectorizer.fit_transform(data[:10000])\n",
        "sklearn_time = time.time() - start_time\n",
        "\n",
        "print(f\"Время выполнения: {sklearn_time:.4f} секунд\")\n",
        "print(f\"Размер матрицы: {sklearn_encoded.shape}\")\n",
        "print(f\"Память: {sklearn_encoded.data.nbytes} байт (разреженное представление)\")\n",
        "print(\"Словарь:\", sklearn_vectorizer.get_feature_names_out()[:5])\n",
        "\n",
        "# Сравниваем результаты\n",
        "print(\"\\n=== Сравнение производительности ===\")\n",
        "comparison = pd.DataFrame({\n",
        "    'Метод': ['Кастомная реализация', 'Scikit-Learn'],\n",
        "    'Время (сек)': [custom_time, sklearn_time],\n",
        "    'Память (байт)': [custom_encoded.nbytes, sklearn_encoded.data.nbytes],\n",
        "    'Размер матрицы': [str(custom_encoded.shape), str(sklearn_encoded.shape)]\n",
        "})\n",
        "\n",
        "print(comparison)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shDzAMYt1OMf",
        "outputId": "1dd7b9a4-db5d-4e84-b817-dc7f0e05a17b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Кастомная реализация One-Hot Encoding ===\n",
            "Размер словаря: 45032 слов\n",
            "Время выполнения: 3.0682 секунд\n",
            "Размер матрицы: (10000, 45032)\n",
            "Память: 3602560000 байт\n",
            "\n",
            "=== Реализация Scikit-Learn ===\n",
            "Время выполнения: 0.2848 секунд\n",
            "Размер матрицы: (10000, 27841)\n",
            "Память: 1092312 байт (разреженное представление)\n",
            "Словарь: ['00' '000' '01' '03' '06']\n",
            "\n",
            "=== Сравнение производительности ===\n",
            "                  Метод  Время (сек)  Память (байт)  Размер матрицы\n",
            "0  Кастомная реализация     3.068161     3602560000  (10000, 45032)\n",
            "1          Scikit-Learn     0.284762        1092312  (10000, 27841)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7amUSXm27-b",
        "outputId": "8686b0ab-d09c-4882-cee3-7c47e86e42d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '01', ..., 'ящура', 'ёеее', 'ёлочкой'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sklearn_encoded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqiua_UW2rf3",
        "outputId": "650c52ee-1790-43f4-84a9-56094a751d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
              "\twith 102 stored elements and shape (17, 74)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#help(CountVectorizer)"
      ],
      "metadata": {
        "id": "ohWH0be33IgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Интересное"
      ],
      "metadata": {
        "id": "kFyNYjtl6TY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Bag_of_words = CountVectorizer(binary=False) # так по умолчанию"
      ],
      "metadata": {
        "id": "a77qnCqr8bTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#help(HashingVectorizer) # для очень больших данных"
      ],
      "metadata": {
        "id": "mfbgDWYK4XvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка кодировщика с продвинутыми параметрами\n",
        "encoder = OneHotEncoder(\n",
        "    categories='auto',\n",
        "    drop='if_binary',           # Удаляет только для бинарных признаков\n",
        "    sparse_output=True,\n",
        "    handle_unknown='infrequent_if_exist',  # Группировка редких категорий\n",
        "    min_frequency=1,            # Минимальная частота категории\n",
        "    max_categories=10,          # Максимальное количество категорий\n",
        "    feature_name_combiner='concat'  # Формирование имен признаков\n",
        ")"
      ],
      "metadata": {
        "id": "-gPJ3ASa6Zyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf-idf\n",
        "\n",
        "На занятии предлагалось разобраться с реализацией tf-idf и дополнив код сравнить с реализацией из библиотеки"
      ],
      "metadata": {
        "id": "wI4ltQXv7GYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "class CustomTfidfVectorizer:\n",
        "    def __init__(self, lowercase=True, remove_stopwords=True, language='russian'):\n",
        "        self.lowercase = lowercase\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.language = language\n",
        "        self.vocabulary_ = None\n",
        "        self.idf_ = None\n",
        "        self.stop_words = set(stopwords.words(language)) if remove_stopwords else set()\n",
        "        self.stemmer = SnowballStemmer(language)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"Предобработка текста\"\"\"\n",
        "        if self.lowercase:\n",
        "            text = text.lower()\n",
        "\n",
        "        # Удаляем знаки препинания и цифры\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "        # Токенизация\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Удаление стоп-слов и стемминг\n",
        "        if self.remove_stopwords:\n",
        "            tokens = [token for token in tokens if token not in self.stop_words]\n",
        "\n",
        "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"Обучение модели - построение словаря и вычисление IDF\"\"\"\n",
        "        # Собираем все токены\n",
        "        all_tokens = []\n",
        "        doc_token_sets = []\n",
        "\n",
        "        for doc in documents:\n",
        "            tokens = self.preprocess_text(doc)\n",
        "            all_tokens.extend(tokens)\n",
        "            doc_token_sets.append(set(tokens))\n",
        "\n",
        "        # Создаем словарь\n",
        "        self.vocabulary_ = {token: idx for idx, token in enumerate(set(all_tokens))}\n",
        "\n",
        "        # Вычисляем IDF\n",
        "        n_docs = len(documents)\n",
        "        self.idf_ = np.zeros(len(self.vocabulary_))\n",
        "\n",
        "        for token, idx in self.vocabulary_.items():\n",
        "            # Количество документов, содержащих токен\n",
        "            doc_count = sum(1 for doc_tokens in doc_token_sets if token in doc_tokens)\n",
        "            # Формула IDF с добавлением 1 для избежания деления на 0\n",
        "            self.idf_[idx] = np.log((n_docs + 1) / (doc_count + 1)) + 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"Преобразование документов в TF-IDF матрицу\"\"\"\n",
        "        n_docs = len(documents)\n",
        "        n_features = len(self.vocabulary_)\n",
        "        tfidf_matrix = np.zeros((n_docs, n_features))\n",
        "\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            tokens = self.preprocess_text(doc)\n",
        "\n",
        "            if not tokens:\n",
        "                continue\n",
        "\n",
        "            # Вычисляем TF (термическая частота)\n",
        "            tf = defaultdict(int)\n",
        "            total_terms = len(tokens)\n",
        "\n",
        "            for token in tokens:\n",
        "                if token in self.vocabulary_:\n",
        "                    tf[token] += 1\n",
        "\n",
        "            # Нормализуем TF\n",
        "            for token in tf:\n",
        "                tf[token] /= total_terms\n",
        "\n",
        "            # Вычисляем TF-IDF\n",
        "            for token, tf_val in tf.items():\n",
        "                if token in self.vocabulary_:\n",
        "                    token_idx = self.vocabulary_[token]\n",
        "                    tfidf_matrix[doc_idx, token_idx] = tf_val * self.idf_[token_idx]\n",
        "\n",
        "        # L2 нормализация\n",
        "        norms = np.linalg.norm(tfidf_matrix, axis=1, keepdims=True)\n",
        "        norms[norms == 0] = 1  # Избегаем деления на 0\n",
        "        tfidf_matrix = tfidf_matrix / norms\n",
        "\n",
        "        return tfidf_matrix\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"Обучение и преобразование\"\"\"\n",
        "        return self.fit(documents).transform(documents)\n",
        "\n",
        "# Тестирование и сравнение\n",
        "def compare_tfidf_implementations():\n",
        "    # Пример данных на русском языке\n",
        "    documents = [\n",
        "        \"кот сидит на ковре и смотрит в окно\",\n",
        "        \"собака бегает по двору и играет с мячом\",\n",
        "        \"кот и собака иногда играют вместе\",\n",
        "        \"птица летает высоко в небе над домом\",\n",
        "        \"рыба плавает в аквариуме и смотрит на камни\"\n",
        "    ]\n",
        "\n",
        "    print(\"Документы для анализа:\")\n",
        "    for i, doc in enumerate(documents, 1):\n",
        "        print(f\"{i}. {doc}\")\n",
        "    print()\n",
        "\n",
        "    # Наша реализация\n",
        "    print(\"=== Наша реализация TF-IDF ===\")\n",
        "    custom_tfidf = CustomTfidfVectorizer()\n",
        "    custom_matrix = custom_tfidf.fit_transform(documents)\n",
        "\n",
        "    print(\"Размерность матрицы:\", custom_matrix.shape)\n",
        "    print(\"Словарь (первые 10 слов):\", dict(list(custom_tfidf.vocabulary_.items())[:10]))\n",
        "    print(\"IDF значения (первые 10):\", custom_tfidf.idf_[:10])\n",
        "    print()\n",
        "\n",
        "    # Реализация sklearn\n",
        "    print(\"=== Sklearn TF-IDF ===\")\n",
        "    sklearn_tfidf = TfidfVectorizer(\n",
        "        lowercase=True,\n",
        "        stop_words=stopwords.words('russian')\n",
        "    )\n",
        "    sklearn_matrix = sklearn_tfidf.fit_transform(documents).toarray()\n",
        "\n",
        "    print(\"Размерность матрицы:\", sklearn_matrix.shape)\n",
        "    print(\"Словарь (первые 10 слов):\", dict(list(sklearn_tfidf.vocabulary_.items())[:10]))\n",
        "    print(\"IDF значения (первые 10):\", sklearn_tfidf.idf_[:10])\n",
        "    print()\n",
        "\n",
        "    # Сравнение результатов\n",
        "    print(\"=== Сравнение результатов ===\")\n",
        "\n",
        "    # Сравнение матриц\n",
        "    difference = np.abs(custom_matrix - sklearn_matrix)\n",
        "    print(f\"Средняя разница между матрицами: {np.mean(difference):.6f}\")\n",
        "    print(f\"Максимальная разница: {np.max(difference):.6f}\")\n",
        "\n",
        "    # Сравнение косинусного сходства\n",
        "    print(\"\\nСравнение косинусного сходства:\")\n",
        "\n",
        "    # Для нашей реализации\n",
        "    custom_similarity = cosine_similarity(custom_matrix)\n",
        "    print(\"Косинусное сходство (наша реализация):\")\n",
        "    print(custom_similarity)\n",
        "\n",
        "    print(\"\\nКосинусное сходство (sklearn):\")\n",
        "    sklearn_similarity = cosine_similarity(sklearn_matrix)\n",
        "    print(sklearn_similarity)\n",
        "\n",
        "    # Разница в сходстве\n",
        "    similarity_diff = np.abs(custom_similarity - sklearn_similarity)\n",
        "    print(f\"\\nСредняя разница в косинусном сходстве: {np.mean(similarity_diff):.6f}\")\n",
        "\n",
        "    return custom_tfidf, sklearn_tfidf, custom_matrix, sklearn_matrix\n",
        "\n",
        "# Дополнительная функция для анализа конкретных слов\n",
        "def analyze_specific_words(custom_tfidf, sklearn_tfidf, documents):\n",
        "    \"\"\"Анализ TF-IDF значений для конкретных слов\"\"\"\n",
        "    print(\"\\n=== Анализ конкретных слов ===\")\n",
        "\n",
        "    test_words = [\"кот\", \"собака\", \"птица\"]\n",
        "\n",
        "    for word in test_words:\n",
        "        print(f\"\\nСлово: '{word}'\")\n",
        "\n",
        "        # В нашей реализации\n",
        "        if word in custom_tfidf.vocabulary_:\n",
        "            custom_idx = custom_tfidf.vocabulary_[word]\n",
        "            print(f\"Наша реализация - IDF: {custom_tfidf.idf_[custom_idx]:.4f}\")\n",
        "        else:\n",
        "            print(f\"Наша реализация - слово не найдено в словаре\")\n",
        "\n",
        "        # В sklearn\n",
        "        if word in sklearn_tfidf.vocabulary_:\n",
        "            sklearn_idx = sklearn_tfidf.vocabulary_[word]\n",
        "            print(f\"Sklearn - IDF: {sklearn_tfidf.idf_[sklearn_idx]:.4f}\")\n",
        "        else:\n",
        "            print(f\"Sklearn - слово не найдено в словаре\")\n"
      ],
      "metadata": {
        "id": "vf5XgJAd7F5A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e48c746a-1cf2-4cd6-9dca-580861250ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tfidf, sklearn_tfidf, custom_matrix, sklearn_matrix = compare_tfidf_implementations()\n",
        "analyze_specific_words(custom_tfidf, sklearn_tfidf, documents)\n",
        "\n",
        "# Демонстрация работы с новыми документами\n",
        "print(\"\\n=== Тест на новых документах ===\")\n",
        "new_docs = [\n",
        "    \"кот играет с собакой в саду\",\n",
        "    \"птица сидит на дереве и поет\"\n",
        "]\n",
        "\n",
        "custom_new = custom_tfidf.transform(new_docs)\n",
        "sklearn_new = sklearn_tfidf.transform(new_docs).toarray()\n",
        "\n",
        "print(\"Наша реализация на новых документах:\")\n",
        "print(custom_new)\n",
        "print(\"\\nSklearn на новых документах:\")\n",
        "print(sklearn_new)"
      ],
      "metadata": {
        "id": "glpoYidv3GFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CzS_NhWehnNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER\n",
        "\n",
        "Задача NER — это извлечение именованных сущностей из текста (например, имена людей, организации, локации, даты и т.д.)  \n",
        "\n",
        "Предлагается изучить какими методами её можно решать\n",
        "\n"
      ],
      "metadata": {
        "id": "yUd0XH9wLUKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pE-82bmQJtgU",
        "outputId": "134571bc-288f-4dfe-9aeb-9e6e156a91fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.17.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.4-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.4-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.4 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "text = data[69]\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(text)\n",
        "\n",
        "# Извлечение сущностей\n",
        "for ent in doc.ents:\n",
        "    print(f\"Сущность: {ent.text}, Тип: {ent.label_}, Описание: {spacy.explain(ent.label_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JFZmT8eLmUL",
        "outputId": "e08c2e1b-ce87-4d01-f3b9-b299840cfa2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сущность: России, Тип: LOC, Описание: Non-GPE locations, mountain ranges, bodies of water\n",
            "Сущность: Антон Силуанов, Тип: PER, Описание: Named person or family.\n",
            "Сущность: Владимиром Путиным, Тип: PER, Описание: Named person or family.\n",
            "Сущность: Москве, Тип: LOC, Описание: Non-GPE locations, mountain ranges, bodies of water\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style='dep', jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "eJm64OeLMpKs",
        "outputId": "adacd168-673d-4b23-97b9-76b2ce436421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"ru\" id=\"1eaff5ce1fb54f999a9ef17fb0fe8520-0\" class=\"displacy\" width=\"3200\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Министр</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">финансов</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">России</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Антон</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Силуанов</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">заявил,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">что</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">встретится</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">с</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">президентом</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">Владимиром</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">Путиным</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">в</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2325\">Москве</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2325\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2500\">25</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2500\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2675\">декабря</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2675\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">2023</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3025\">года.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3025\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 925.0,2.0 925.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,264.5 210.0,264.5 210.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M210.0,354.0 L218.0,342.0 202.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-2\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-3\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,354.0 L578.0,342.0 562.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">flat:name</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M735.0,354.0 L743.0,342.0 727.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-6\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1265.0,354.0 L1273.0,342.0 1257.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-7\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,264.5 1610.0,264.5 1610.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470,354.0 L1462,342.0 1478,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-8\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,177.0 1615.0,177.0 1615.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1615.0,354.0 L1623.0,342.0 1607.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-9\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,264.5 1785.0,264.5 1785.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1785.0,354.0 L1793.0,342.0 1777.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-10\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">flat:name</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1960.0,354.0 L1968.0,342.0 1952.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-11\" stroke-width=\"2px\" d=\"M2170,352.0 C2170,264.5 2310.0,264.5 2310.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2170,354.0 L2162,342.0 2178,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-12\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,89.5 2320.0,89.5 2320.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2320.0,354.0 L2328.0,342.0 2312.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-13\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,2.0 2500.0,2.0 2500.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2500.0,354.0 L2508.0,342.0 2492.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-14\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,264.5 2660.0,264.5 2660.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">flat</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2660.0,354.0 L2668.0,342.0 2652.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-15\" stroke-width=\"2px\" d=\"M2870,352.0 C2870,264.5 3010.0,264.5 3010.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M2870,354.0 L2862,342.0 2878,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-16\" stroke-width=\"2px\" d=\"M2520,352.0 C2520,177.0 3015.0,177.0 3015.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-1eaff5ce1fb54f999a9ef17fb0fe8520-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M3015.0,354.0 L3023.0,342.0 3007.0,342.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "text = data[69]\n",
        "tokens = text.split()\n",
        "\n",
        "for token in tokens:\n",
        "    parsed = morph.parse(token)[0]\n",
        "    print(f\"Слово: {token}, Нормальная форма: {parsed.normal_form}, Часть речи: {parsed.tag.POS}\")"
      ],
      "metadata": {
        "id": "oKkHQueMLt5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS\n",
        "\n",
        "Задача POS — это определение частеречной принадлежности слов в тексте (существительное, глагол, прилагательное и т.д.).\n",
        "\n",
        "Предлагается изучить какими методами её можно решать"
      ],
      "metadata": {
        "id": "ZwzxMMKpLWyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Загрузка модели для русского языка\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "text = \"Красивая кошка быстро бежала по зеленому полю и ловила мышку\"\n",
        "\n",
        "# Обработка текста\n",
        "doc = nlp(text)\n",
        "\n",
        "# POS-разметка\n",
        "print(\"POS-разметка с помощью spacy:\")\n",
        "print(\"-\" * 50)\n",
        "for token in doc:\n",
        "    print(f\"Слово: {token.text:12} POS-тег: {token.pos_:8} Описание: {spacy.explain(token.pos_)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcFlg9YCJwFh",
        "outputId": "5df8501a-11e7-4d12-9791-69377ef89e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS-разметка с помощью spacy:\n",
            "--------------------------------------------------\n",
            "Слово: Красивая     POS-тег: ADJ      Описание: adjective\n",
            "Слово: кошка        POS-тег: NOUN     Описание: noun\n",
            "Слово: быстро       POS-тег: ADV      Описание: adverb\n",
            "Слово: бежала       POS-тег: VERB     Описание: verb\n",
            "Слово: по           POS-тег: ADP      Описание: adposition\n",
            "Слово: зеленому     POS-тег: ADJ      Описание: adjective\n",
            "Слово: полю         POS-тег: NOUN     Описание: noun\n",
            "Слово: и            POS-тег: CCONJ    Описание: coordinating conjunction\n",
            "Слово: ловила       POS-тег: VERB     Описание: verb\n",
            "Слово: мышку        POS-тег: NOUN     Описание: noun\n",
            "Слово: .            POS-тег: PUNCT    Описание: punctuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "text = \"Красивая кошка быстро бежала по зеленому полю\"\n",
        "tokens = text.split()\n",
        "\n",
        "print(\"POS-разметка с помощью pymorphy3:\")\n",
        "print(\"-\" * 50)\n",
        "for token in tokens:\n",
        "    parsed = morph.parse(token)[0]  # Берем наиболее вероятный разбор\n",
        "    pos_tag = parsed.tag.POS\n",
        "    full_grammar = parsed.tag\n",
        "    normal_form = parsed.normal_form\n",
        "\n",
        "    print(f\"Слово: {token:12} POS: {pos_tag:8} Нормальная форма: {normal_form:12} Грамматика: {full_grammar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-8rcrhVNWVl",
        "outputId": "7b9eb5e3-8f59-4f72-f1c5-572e84c63819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS-разметка с помощью pymorphy3:\n",
            "--------------------------------------------------\n",
            "Слово: Красивая     POS: ADJF     Нормальная форма: красивый     Грамматика: ADJF,Qual femn,sing,nomn\n",
            "Слово: кошка        POS: NOUN     Нормальная форма: кошка        Грамматика: NOUN,anim,femn sing,nomn\n",
            "Слово: быстро       POS: ADVB     Нормальная форма: быстро       Грамматика: ADVB\n",
            "Слово: бежала       POS: VERB     Нормальная форма: бежать       Грамматика: VERB,perf,intr femn,sing,past,indc\n",
            "Слово: по           POS: PREP     Нормальная форма: по           Грамматика: PREP\n",
            "Слово: зеленому     POS: ADJF     Нормальная форма: зелёный      Грамматика: ADJF,Qual masc,sing,datv\n",
            "Слово: полю         POS: NOUN     Нормальная форма: поле         Грамматика: NOUN,inan,neut sing,datv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Классификация (регулярки)"
      ],
      "metadata": {
        "id": "3nFOo-t2LXwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# найдите все анекдоты про Вовочку в датасете\n",
        "# предлагается рассмотреть два варианта\n",
        "# - через регулярные выражения (не стоит забывать об этом мощном способе)\n",
        "# - через поиск эмбеддинга к слову \"Вовочка\""
      ],
      "metadata": {
        "id": "UVtmNRpKJ6Z7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}