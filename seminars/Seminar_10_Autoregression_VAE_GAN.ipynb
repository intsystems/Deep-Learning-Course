{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e15c97",
   "metadata": {},
   "source": [
    "# Seminar 10: Autoregression, VAE, and GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b9be1d",
   "metadata": {},
   "source": [
    "**Deep Learning Course 2025**\n",
    "\n",
    "**Author:** Nikita Kiselev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f481a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.5\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432b229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38daa9f8",
   "metadata": {},
   "source": [
    "## 1. Autoregression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4151c32e",
   "metadata": {},
   "source": [
    "Let's use binarized MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: (x > 0.5).float()\n",
    "])\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4239a",
   "metadata": {},
   "source": [
    "**Building the model**\n",
    "\n",
    "Image pixels are treated as input tokens. Their values can be either 0 or 1.\n",
    "\n",
    "To leverage autoregressive modeling, we add `<START>` token into beginning, let it equals 2.\n",
    "\n",
    "As we follow classification objective and use tokens from predefined vocabulary (0, 1, 2), we need embeddings for them.\n",
    "\n",
    "Recurrent neural network is implemented with LSTM module.\n",
    "\n",
    "The last layer projects hidden states into probabilities of ones per each pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e63e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelRNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(3, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        start_token = torch.full((x.shape[0], 1), 2, dtype=torch.long, device=x.device)\n",
    "        x = torch.cat([start_token, x[:, :-1]], dim=1)\n",
    "        x = self.embed(x)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c1686",
   "metadata": {},
   "source": [
    "**Training and evaluating the model**\n",
    "\n",
    "We've alredy build the model class, so let's train it!\n",
    "\n",
    "Firstly, we define a loss function – Binary Cross Entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f0de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea0b2d6",
   "metadata": {},
   "source": [
    "Then, we make a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a63ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 3e-4\n",
    "\n",
    "model = PixelRNN().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    for x, _ in train_loader:\n",
    "        x = x.view(-1, 784).to(device)\n",
    "        prob = model(x.long())\n",
    "        loss = criterion(prob.view(-1), x.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd57ac",
   "metadata": {},
   "source": [
    "**Sampling from the latent space**\n",
    "\n",
    "We’ll sample random points from a standard normal distribution and feed them through the decoder to generate synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd881f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, num_samples):\n",
    "    model.eval()\n",
    "    x = torch.full((num_samples, 1), 2, dtype=torch.long, device=device)\n",
    "    h = c = None\n",
    "    generated = []\n",
    "\n",
    "    for _ in range(784):\n",
    "        emb = model.embed(x)\n",
    "        if h is None or c is None:\n",
    "            out, (h, c) = model.rnn(emb)\n",
    "        else:\n",
    "            out, (h, c) = model.rnn(emb, (h, c))\n",
    "        logits = model.fc(out)\n",
    "        p = torch.sigmoid(logits).squeeze(-1)\n",
    "        x = torch.bernoulli(p).long()\n",
    "        generated.append(x)\n",
    "\n",
    "    images = torch.stack(generated, dim=1)\n",
    "    images = images.view(num_samples, 28, 28).float().cpu()\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ad568",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 16\n",
    "\n",
    "images = sample(model, num_samples)\n",
    "    \n",
    "fig, axes = plt.subplots(2, num_samples // 2, figsize=(12, 4)) \n",
    "for i, ax in enumerate(axes.flat): \n",
    "  ax.imshow(images[i], cmap='gray') \n",
    "  ax.axis('off') \n",
    "\n",
    "plt.suptitle(\"Generated Samples\")\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0fdb71",
   "metadata": {},
   "source": [
    "## 2. VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1278f",
   "metadata": {},
   "source": [
    "In contrast to PixelRNN example, here we use **non-binarized** MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e21a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e1ba4",
   "metadata": {},
   "source": [
    "**Building the encoder class**\n",
    "\n",
    "The encoder takes an input image and compresses it into a compact latent representation.\n",
    "\n",
    "But unlike a regular autoencoder, it doesn’t output a single point - instead, it outputs two vectors: the mean $\\boldsymbol{\\mu}$ and log-variance $\\log \\boldsymbol{\\sigma}^2$.\n",
    "\n",
    "These define a probability distribution from which we’ll later sample a latent vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86654ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44714e16",
   "metadata": {},
   "source": [
    "**Implementing the decoder class**\n",
    "\n",
    "The decoder does the reverse, it takes a sampled point from the latent space and tries to reconstruct the original image.\n",
    "\n",
    "This helps the VAE learn to generate new data similar to the training inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d3aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=400, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc686",
   "metadata": {},
   "source": [
    "**Creating the main VAE class**\n",
    "\n",
    "The VAE class combines the encoder and decoder and implements the reparameterization trick to keep training differentiable.\n",
    "\n",
    "Since sampling directly from a distribution $\\mathbf{z} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\sigma}^2)$ breaks backpropagation, we sample $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ and compute $\\mathbf{z} = \\boldsymbol{\\mu} + \\boldsymbol{\\sigma} \\cdot  \\boldsymbol{\\epsilon}$ instead.\n",
    "\n",
    "This trick allows gradients to flow through the sampling step during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40e5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(mu)\n",
    "        z = mu + std * eps\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5c9fd5",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "Training a VAE involves optimizing a composite loss function that balances two goals:\n",
    "\n",
    "1. **Reconstruction Loss:** Ensures the output is close to the original input.\n",
    "\n",
    "2. **Regularization Loss (KL divergence):** Encourages the learned latent distribution to be close to a standard normal distribution.\n",
    "\n",
    "The total loss is often referred to as the ELBO (Evidence Lower Bound), and we aim to maximize it (or equivalently, minimize its negative).\n",
    "\n",
    "In our case (binary images from MNIST) we have the following reconstruction loss:\n",
    "\n",
    "$$ \\mathcal{L}_2(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = \\| \\mathbf{x} - \\mathbf{x}' \\|_2^2 $$\n",
    "\n",
    "Regularization loss is easily derived for gaussian latent distribution as follows:\n",
    "\n",
    "$$ \\mathcal{L}_{\\text{KL}}(\\boldsymbol{\\theta}, \\boldsymbol{\\phi}) = - \\frac{1}{2} \\left( 1 + \\log \\boldsymbol{\\sigma}^2 - \\boldsymbol{\\mu}^2 - \\boldsymbol{\\sigma}^2 \\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c145255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_recon, mu, logvar):\n",
    "    recon_loss = F.mse_loss(x_recon, x, reduction=\"sum\")\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    total_loss = recon_loss + kl_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f37ccd4",
   "metadata": {},
   "source": [
    "**Training and evaluating the VAE**\n",
    "\n",
    "Now that we’ve defined the VAE model and its loss function, it’s time to train it.\n",
    "\n",
    "We’ll run the training loop for several epochs, calculate the loss, and visualize how well the VAE learns to reconstruct and generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = VAE().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    for x, _ in train_loader:\n",
    "        x = x.view(-1, 784).to(device)\n",
    "        x_recon, mu, logvar = model(x)\n",
    "        loss = loss_function(x, x_recon, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ade032",
   "metadata": {},
   "source": [
    "**Sampling from the latent space**\n",
    "\n",
    "We’ll sample random points from a standard normal distribution and feed them through the decoder to generate synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, num_samples):\n",
    "    model.eval()\n",
    "    z = torch.randn(num_samples, 20).to(device)\n",
    "    x = model.decoder(z).cpu()\n",
    "    x = x.view(-1, 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b18577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 16\n",
    "\n",
    "x = sample(model, num_samples)\n",
    "    \n",
    "fig, axes = plt.subplots(2, num_samples // 2, figsize=(12, 4)) \n",
    "for i, ax in enumerate(axes.flat): \n",
    "  ax.imshow(x[i][0], cmap='gray') \n",
    "  ax.axis('off') \n",
    "\n",
    "plt.suptitle(\"Generated Samples from Latent Space\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c66b96",
   "metadata": {},
   "source": [
    "**Latent space interpolation**\n",
    "\n",
    "We can also interpolate between two points in the latent space to see smooth transitions in generated images, which is a hallmark of a well-trained VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57983d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interpolate(model, z1, z2, steps=10):\n",
    "    model.eval()\n",
    "    z = torch.stack([\n",
    "        z1 * (1 - t) + z2 * t for t in torch.linspace(0, 1, steps)\n",
    "    ]).to(device)\n",
    "    x = model.decoder(z.to(device)).cpu()\n",
    "    x = x.view(-1, 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.randn(1, 20)\n",
    "z2 = torch.randn(1, 20)\n",
    "steps = 10\n",
    "\n",
    "x = interpolate(model, z1, z2, steps)\n",
    "\n",
    "fig, axes = plt.subplots(1, steps, figsize=(15, 2))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "  ax.imshow(x[i][0], cmap=\"gray\")\n",
    "  ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Latent Space Interpolation\")  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5950aaf",
   "metadata": {},
   "source": [
    "## 3. GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd048d37",
   "metadata": {},
   "source": [
    "Similar to VAE example, here we use **non-binarized** MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f3a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c4fff",
   "metadata": {},
   "source": [
    "**Building the Generator class**\n",
    "\n",
    "Generator $G$ maps random noise $\\mathbf{z} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ to images and tries to fool the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954206e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=20, hidden_dim=400, output_dim=784):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5971f9a",
   "metadata": {},
   "source": [
    "**Implementing the Discriminator class**\n",
    "\n",
    "Discriminator $D$ receives an image and outputs a probability that it is real (from data) rather than fake (from $G$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc534d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.act1 = nn.GELU()\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.fc1(x))\n",
    "        x = self.act2(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75408caf",
   "metadata": {},
   "source": [
    "**Loss function**\n",
    "\n",
    "We formalize GAN's training with a minimax objective:\n",
    "$$\n",
    "\\min_{G} \\max_{D} \\left[ \\mathbb{E}_{\\pi(\\mathbf{x})} \\log D(\\mathbf{x}) + \\mathbb{E}_{p(\\mathbf{z})} \\log (1 - D(G(\\mathbf{z}))) \\right]\n",
    "$$\n",
    "\n",
    "Consider a binary cross entropy (BCE):\n",
    "$$\n",
    "\\text{BCE}(\\hat{y}, y) = - \\left[ y \\cdot \\log \\hat{y} + (1 - y) \\cdot \\log (1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Using BCE, the discriminator loss (calculated on minibatch) becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_D = \\text{BCE}(D(\\mathbf{x}), 1) + \\text{BCE}(D(G(\\mathbf{z})), 0)\n",
    "$$\n",
    "\n",
    "If we followed the original minimax formula, the generator would minimize:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p(\\mathbf{z})} \\log (1 - D(G(\\mathbf{z})))\n",
    "$$\n",
    "\n",
    "But this saturates when $D$ is good at rejecting fakes, leading to weak gradients.\n",
    "\n",
    "Instead, we use the **non-saturating trick**: maximize $\\log D(G(\\mathbf{z}))$, or in BCE minibatch notation:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_G = \\text{BCE}(D(G(\\mathbf{z})), 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca6e1a8",
   "metadata": {},
   "source": [
    "**Training and evaluating the GAN**\n",
    "\n",
    "Now that we’ve defined the GAN model, so it’s time to train it.\n",
    "\n",
    "We’ll run the training loop for several epochs, calculate the loss, and visualize how well the GAN learns to generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "optimizer_generator = optim.AdamW(generator.parameters(), lr=learning_rate)\n",
    "optimizer_discriminator = optim.AdamW(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "generator.train()\n",
    "discriminator.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    for x_real, _ in train_loader:\n",
    "        x_real = x_real.view(-1, 784).to(device)\n",
    "        bs = x_real.shape[0]\n",
    "        \n",
    "        labels_real = torch.ones(bs, 1).to(device)\n",
    "        labels_fake = torch.zeros(bs, 1).to(device)\n",
    "        \n",
    "        # Discriminator step\n",
    "        y_real = discriminator(x_real)\n",
    "        loss_discriminator_real = criterion(y_real, labels_real)\n",
    "        \n",
    "        z = torch.randn(bs, 20).to(device)\n",
    "        x_fake = generator(z).detach()  # NOTE: stop gradient for generator here\n",
    "        y_fake = discriminator(x_fake)\n",
    "        loss_discriminator_fake = criterion(y_fake, labels_fake)\n",
    "        \n",
    "        loss_discriminator = loss_discriminator_real + loss_discriminator_fake\n",
    "        loss_discriminator.backward()\n",
    "        optimizer_discriminator.step()\n",
    "        optimizer_discriminator.zero_grad()\n",
    "        \n",
    "        # Generator step\n",
    "        z = torch.randn(bs, 20).to(device)\n",
    "        x_fake = generator(z)\n",
    "        \n",
    "        y_fake = discriminator(x_fake)\n",
    "        loss_generator = criterion(y_fake, labels_real)\n",
    "        loss_generator.backward()\n",
    "        optimizer_generator.step()\n",
    "        optimizer_generator.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cd3f21",
   "metadata": {},
   "source": [
    "**Sampling from the latent space**\n",
    "\n",
    "We’ll sample random points from a standard normal distribution and feed them through the generator to generate synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(generator, num_samples):\n",
    "    generator.eval()\n",
    "    z = torch.randn(num_samples, 20).to(device)\n",
    "    x = generator(z).cpu()\n",
    "    x = x.view(-1, 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 16\n",
    "\n",
    "x = sample(generator, num_samples)\n",
    "    \n",
    "fig, axes = plt.subplots(2, num_samples // 2, figsize=(12, 4)) \n",
    "for i, ax in enumerate(axes.flat): \n",
    "  ax.imshow(x[i][0], cmap='gray') \n",
    "  ax.axis('off') \n",
    "\n",
    "plt.suptitle(\"Generated Samples from Latent Space\") \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58222028",
   "metadata": {},
   "source": [
    "**Latent space interpolation**\n",
    "\n",
    "We can also interpolate between two points in the latent space to see smooth transitions in generated images, which is a hallmark of a well-trained VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b585579",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interpolate(generator, z1, z2, steps=10):\n",
    "    generator.eval()\n",
    "    z = torch.stack([\n",
    "        z1 * (1 - t) + z2 * t for t in torch.linspace(0, 1, steps)\n",
    "    ]).to(device)\n",
    "    x = generator(z.to(device)).cpu()\n",
    "    x = x.view(-1, 1, 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dce9773",
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.randn(1, 20)\n",
    "z2 = torch.randn(1, 20)\n",
    "steps = 10\n",
    "\n",
    "x = interpolate(generator, z1, z2, steps)\n",
    "\n",
    "fig, axes = plt.subplots(1, steps, figsize=(15, 2))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "  ax.imshow(x[i][0], cmap=\"gray\")\n",
    "  ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Latent Space Interpolation\")  \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kandinsky-cuda12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
