{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ec4733",
   "metadata": {},
   "source": [
    "# Seminar 11: Flow Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d532ca",
   "metadata": {},
   "source": [
    "**Deep Learning Course 2025**\n",
    "\n",
    "**Author:** Nikita Kiselev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c36d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "plt.rcParams[\"axes.linewidth\"] = 1.5\n",
    "plt.rcParams[\"font.size\"] = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747282af",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6970fb3f",
   "metadata": {},
   "source": [
    "**2D-Spiral distribution**\n",
    "\n",
    "In this section we will train a Flow Matching model to generate data from a 2D-Spiral distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spiral_data(num_samples=100_000, scale=1):\n",
    "    noise = 0.1 * scale \n",
    "    theta = 6 * torch.pi * torch.rand(num_samples)\n",
    "    r = theta / (2 * torch.pi) * scale\n",
    "    x = r * torch.cos(theta) + noise * torch.randn(num_samples)\n",
    "    y = r * torch.sin(theta) + noise * torch.randn(num_samples)\n",
    "    return torch.stack([x, y], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_spiral_data()\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(train_dataset[:1000, 0], train_dataset[:1000, 1], s=50, alpha=0.5, color=\"green\")\n",
    "plt.title(\"Data Distribution\")\n",
    "plt.grid(alpha=0.1)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-3.5, 3.5)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01a602d",
   "metadata": {},
   "source": [
    "**Flow Matching (Linear Interpolation)**\n",
    "\n",
    "In conditional flow matching, our objective is to learn a vector field $\\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t)$, parameterized by a neural network, that aligns with a known target vector field $\\mathbf{f}(\\mathbf{x}, \\mathbf{z}, t)$ at each point along a path connecting the data distribution and a base distribution. In this task, we consider the **linear interpolation conditional vector field**, defined by:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}, \\mathbf{z}, t) = \\frac{d\\mathbf{x}}{dt} = \\mathbf{x}_1 - \\mathbf{x}_0\n",
    "$$\n",
    "which means that $\\mathbf{x}$ iterpolates linearly by making data from pure noise:\n",
    "$$\n",
    "\\mathbf{x}_t = t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_0\n",
    "$$\n",
    "\n",
    "So, the training objective is defined as:\n",
    "$$\n",
    "\\mathbb{E}_{t \\sim \\mathcal{U}[0, 1]}\\, \\mathbb{E}_{\\mathbf{x}_1 \\sim \\pi(\\mathbf{x})} \\mathbb{E}_{\\mathbf{x}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})} \\left[ \\left\\| (\\mathbf{x}_1 - \\mathbf{x}_0) - \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}, t) \\right\\|^2 \\right] \\to \\min_{\\boldsymbol{\\theta}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f31dec1",
   "metadata": {},
   "source": [
    "**Building the model class**\n",
    "\n",
    "Our Flow Matching model must take two arguments: noisy vector $\\mathbf{x}_t$ and timestep $t$.\n",
    "\n",
    "Here we map them into a common hidden space and add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowMatchingModel(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=512):\n",
    "        super().__init__()\n",
    "        self.x_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        self.t_proj = TimeEmbedding(hidden_dim)  # TODO: need to be implemented\n",
    "        self.net = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        x = self.x_proj(x)\n",
    "        t = self.t_proj(t)\n",
    "        x = x + t\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86577629",
   "metadata": {},
   "source": [
    "To condition on a timestep we need to implement a time embedding module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890a3828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        # self.freqs = torch.arange(1, dim // 2 + 1) * torch.pi\n",
    "        self.register_buffer(\"freqs\", torch.arange(1, dim // 2 + 1) * torch.pi)  # NOTE: important for .to(device), etc.\n",
    "        \n",
    "    def forward(self, t):\n",
    "        emb = self.freqs * t\n",
    "        emb = torch.cat([emb.cos(), emb.sin()], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a9b512",
   "metadata": {},
   "source": [
    "**Training and evaluating the Flow Matching model**\n",
    "\n",
    "Now that we’ve defined the Flow Matching model, it’s time to train it.\n",
    "\n",
    "We’ll run the training loop for several epochs, calculate the loss, and visualize how well the model learns to generate data.\n",
    "\n",
    "Our training loop must follow the next steps:\n",
    "1. Sample $\\mathbf{x}_1 \\sim \\pi(\\mathbf{x})$\n",
    "2. Sample time $t \\sim \\mathcal{U}[0, 1]$ and $\\mathbf{x}_0 \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$\n",
    "3. Obtain the noisy vector $\\mathbf{x}_t = t \\mathbf{x}_1 + (1 - t) \\mathbf{x}_0$\n",
    "4. Compute the loss $\\mathcal{L} = \\left\\| (\\mathbf{x}_1 - \\mathbf{x}_0) - \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_t, t) \\right\\|_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = FlowMatchingModel().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(epochs), desc=\"Epoch\"):\n",
    "    for x_1 in train_loader:\n",
    "        x_1 = x_1.to(device)\n",
    "        x_0 = torch.randn_like(x_1).to(device)\n",
    "        t = torch.rand(x_1.shape[0], 1).to(device)\n",
    "        x_t = t * x_1 + (1 - t) * x_0\n",
    "        velocity = x_1 - x_0\n",
    "        pred_velocity = model(x_t, t)\n",
    "        loss = F.mse_loss(pred_velocity, velocity)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1daa86",
   "metadata": {},
   "source": [
    "**Sample from trained model**\n",
    "\n",
    "To sample from a pretrained Flow Matching model, we need to implement an $\\texttt{ODESolver}$, which can be a simple Euler method:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(t + h) = \\mathbf{x}(t) + h \\cdot \\mathbf{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}(t), t)\n",
    "$$\n",
    "\n",
    "It must take a pretrained model, number of points to sample and a number of approximation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1fa119",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def sample(model, num_samples, num_steps=100):\n",
    "    model.eval()\n",
    "    x = torch.randn(num_samples, 2).to(device)\n",
    "    ts = torch.linspace(0, 1, num_steps).to(device)\n",
    "    for t, dt in zip(ts[:-1], torch.diff(ts)):\n",
    "        t = torch.full((num_samples, 1), t).to(device)\n",
    "        pred_velocity = model(x, t)\n",
    "        x = x + dt * pred_velocity\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample(model, num_samples=1000).cpu()\n",
    "plt.scatter(samples[:, 0], samples[:, 1], s=50, alpha=0.5, color=\"orange\")\n",
    "plt.title(\"Learned Distribution\")\n",
    "plt.grid(alpha=0.1)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-3.5, 3.5)\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb93f95",
   "metadata": {},
   "source": [
    "**Plot transition of the points**\n",
    "\n",
    "Below we provide a code, similar to above one, which will plot the transition of the points from the base distribution to the learned one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc05e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_color(t, start=\"blue\", end=\"orange\"):\n",
    "    start_color = plt.cm.colors.to_rgb(start)\n",
    "    end_color = plt.cm.colors.to_rgb(end)\n",
    "    return (1 - t) * np.array(start_color) + t * np.array(end_color)\n",
    "\n",
    "@torch.no_grad()\n",
    "def plot_transition(model, num_samples, num_steps=100, num_plots=5):\n",
    "    model.eval()\n",
    "    x = torch.randn(num_samples, 2).to(device)\n",
    "    ts = torch.linspace(0, 1, num_steps + 1).to(device)\n",
    "    fig, axes = plt.subplots(1, num_plots, figsize=(4 * num_plots, 4))\n",
    "    plot_idx = 0\n",
    "    ax = axes[plot_idx]\n",
    "    color = interpolate_color(0)\n",
    "    ax.scatter(x.cpu()[:, 0], x.cpu()[:, 1], s=50, alpha=0.5, c=[color])\n",
    "    ax.set_title(f\"t = {0:.2f}\")\n",
    "    ax.grid(alpha=0.1)\n",
    "    ax.set_aspect(\"equal\")\n",
    "    ax.set_xlim(-3.5, 3.5)\n",
    "    ax.set_ylim(-3.5, 3.5)\n",
    "    plot_idx += 1\n",
    "    plot_every = num_steps // (num_plots - 1)\n",
    "    for step, (t, dt) in enumerate(zip(ts[:-1], torch.diff(ts))):\n",
    "        color = interpolate_color(t.cpu())\n",
    "        t = torch.full((num_samples, 1), t).to(device)\n",
    "        pred_velocity = model(x, t)\n",
    "        x = x + dt * pred_velocity\n",
    "        if (step + 1) % plot_every == 0:\n",
    "            ax = axes[plot_idx]\n",
    "            ax.scatter(x.cpu()[:, 0], x.cpu()[:, 1], s=50, alpha=0.5, c=[color])\n",
    "            ax.set_title(f\"t = {t[0][0].cpu() + dt:.2f}\")\n",
    "            ax.grid(alpha=0.1)\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xlim(-3.5, 3.5)\n",
    "            ax.set_ylim(-3.5, 3.5)\n",
    "            plot_idx += 1\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6532dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transition(model, num_samples=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kandinsky-cuda12.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
